{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Drmcoelho/APILLMML/blob/main/bermbriollmogy_portfolio_v2final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5f0d64a",
      "metadata": {
        "id": "c5f0d64a"
      },
      "source": [
        "# Bermbriollmogy Portfolio v2 – Avançado\n",
        "Este notebook-portfólio **v2** reúne um fluxo completo e robusto, integrando:\n",
        "- Ingestão multimodal (PDFs, imagens, áudio, FHIR, web)\n",
        "- Pré-processamento avançado (paralelismo, caching, lematização)\n",
        "- Embeddings comparativos (múltiplos encoders e visualizações)\n",
        "- Data Augmentation e adversarial tests\n",
        "- Fine‑tuning básico e multitarefa com avaliação robusta\n",
        "- RAG multimodal (texto+imagem) e chain‑of‑thought\n",
        "- Otimização, quantização, pruning e benchmarking automático\n",
        "- Containerização, API REST, CI/CD, Kubernetes/HPA\n",
        "- Monitoramento completo e alertas (W&B, Prometheus, Grafana, Slack)\n",
        "- Active Learning, feedback loop e versionamento de datasets/modelos\n",
        "- Governança, compliance, relatórios e documentação automatizada\n",
        "\n",
        "Use o índice abaixo para navegar:\n",
        "1. Setup & Credenciais\n",
        "2. Ingestão Multimodal Avançada\n",
        "3. Pré-processamento Paralelo & Cache\n",
        "4. Embeddings & Similaridade Multimodelo\n",
        "5. EDA, Augmentation & Adversarial\n",
        "6. Fine‑Tuning Básico & Multi‑Task\n",
        "7. RAG Multimodal\n",
        "8. Otimização, Quantização & Pruning\n",
        "9. Containerização & API REST\n",
        "10. CI/CD & Kubernetes\n",
        "11. Monitoramento & Alertas\n",
        "12. Active Learning Avançado\n",
        "13. Governança, Compliance & Documentos\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be336d4b",
      "metadata": {
        "id": "be336d4b"
      },
      "source": [
        "----\n",
        "## Seção 1b/14: Gestão de Credenciais\n",
        "Nesta subseção, você vai inserir de forma segura as credenciais e tokens para serviços:\n",
        "- OpenAI API Key\n",
        "- Hugging Face Token\n",
        "- Gemini API Key\n",
        "- Vertex AI Key\n",
        "- Google Cloud Service Account (JSON)\n",
        "- Slack Webhook URL\n",
        "\n",
        "**Interatividade**: preencha seus segredos e salve em `config.json` ou variáveis de ambiente."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9362f03a",
      "metadata": {
        "id": "9362f03a"
      },
      "source": [
        "----\n",
        "## Seção 1c/14: Quickstart & Verificação de Ambiente\n",
        "Teste imediato do seu ambiente e faça um \"Hello World\" via LLM:\n",
        "- Verifique versões de Python, PyTorch, Transformers e aceleração.\n",
        "- Teste disponibilidade de GPU/CPU.\n",
        "- Execute um prompt simples (\"Olá Mundo\") em OpenAI ou Hugging Face.\n",
        "\n",
        "**Interatividade**: selecione o provedor e execute todos os checks e testes."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2160ca00",
      "metadata": {
        "id": "2160ca00"
      },
      "source": [
        "----\n",
        "## Seção 2a/14: Ingestão Multimodal Avançada – Imagens & Áudio\n",
        "Nesta subseção, você poderá:\n",
        "- Fazer upload e pré-processamento de **imagens médicas** (DICOM, PNG, JPEG).\n",
        "- Transcrever **áudios** clínicos (exames, entrevistas) usando Whisper.\n",
        "- Configurar parâmetros de resizing, normalização e segmentação de áudio.\n",
        "\n",
        "**Interatividade**: selecione arquivos de imagem e áudio, ajuste resolução e taxa de amostragem."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9dffacf",
      "metadata": {
        "id": "e9dffacf"
      },
      "source": [
        "----\n",
        "## Seção 2b/14: Ingestão Avançada – PDFs Longos, FHIR JSONs & Web Scraping\n",
        "Nesta subseção, você poderá:\n",
        "- Processar PDFs de até 10k páginas em chunks paralelizados.\n",
        "- Carregar múltiplos JSONs FHIR, validar e normalize recursos.\n",
        "- Realizar web scraping assíncrono de múltiplas URLs médicas.\n",
        "\n",
        "**Interatividade**: configure paralelismo, chunk size, faça upload de arquivos PDF e FHIR JSONs, e insira URLs."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5656fd3c",
      "metadata": {
        "id": "5656fd3c"
      },
      "source": [
        "----\n",
        "## Seção 3a/14: Pré-processamento Paralelo & Cache\n",
        "Nesta subseção avançada, você poderá:\n",
        "- Aplicar pré-processamento em paralelo (tokenização, limpeza) usando múltiplos processos.\n",
        "- Utilizar cache para evitar reprocessamento de dados já limpos.\n",
        "- Definir pipeline de steps (normalização, stopwords, stemming/lemmatização).\n",
        "\n",
        "**Interatividade**: carregue seu corpus (TXT/CSV), configure número de workers, selecione steps e habilite cache."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2d2bc42",
      "metadata": {
        "id": "a2d2bc42"
      },
      "source": [
        "----\n",
        "## Seção 3b/14: Anotação e Desidentificação de Texto Clínico\n",
        "Neste estágio, iremos:\n",
        "- Remover informações de identificação pessoal (PHI) via regex e NER.\n",
        "- Expandir acrônimos médicos conforme dicionário customizável.\n",
        "- Destacar entidades médicas (medicamentos, doenças) usando spaCy.\n",
        "\n",
        "**Interatividade**: carregue seu texto, defina regras de desidentificação e mapeamento de acrônimos."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80a52e06",
      "metadata": {
        "id": "80a52e06"
      },
      "source": [
        "----\n",
        "## Seção 3c/14: Tokenização Multimodal & Dataset Unificado\n",
        "Nesta subseção, vamos:\n",
        "- Carregar dados processados de múltiplas modalidades:\n",
        "  - Texto clínico (TXT/CSV)\n",
        "  - Transcrições de áudio (JSON)\n",
        "  - Legendas de imagens (CSV/JSON)\n",
        "  - Recursos FHIR (JSON)\n",
        "- Definir proporções de split para treino, validação e teste.\n",
        "- Construir um dataset unificado e salvar em `train.json`, `valid.json`, `test.json`.\n",
        "- Executar testes de integridade (IDs únicos, campos obrigatórios).\n",
        "\n",
        "**Interatividade**: selecione arquivos, ajuste ratios e clique em construir dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "faae946c",
      "metadata": {
        "id": "faae946c"
      },
      "source": [
        "----\n",
        "## Seção 4a/14: Embeddings & Similaridade Multimodelo\n",
        "Nesta subseção, gere e compare embeddings de sentenças clínicas usando múltiplos provedores:\n",
        "- **HuggingFace** (modelos locais, ex.: `all-MiniLM-L6-v2`, `biobert-base-cased-v1.1`)\n",
        "- **OpenAI** (text-embedding-ada-002)\n",
        "- **Vertex AI** (textembedding-gecko)\n",
        "\n",
        "**Funcionalidades:**\n",
        "- Upload de CSV/TXT com sentenças ou input manual.\n",
        "- Seleção de provedores de embedding e parâmetros (batch size, truncation).\n",
        "- Cálculo de similaridade por cosseno e comparação de top‑k entre provedores.\n",
        "- Visualização interativa (bar chart, scatter) para comparar scores.\n",
        "\n",
        "**Interatividade**: escolha provedores, faça upload do corpus e visualize comparações."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12e495c7",
      "metadata": {
        "id": "12e495c7"
      },
      "source": [
        "----\n",
        "## Seção 4b/14: Visualização Comparativa de Embeddings\n",
        "Aqui, compararemos embeddings de diferentes provedores:\n",
        "- Heatmap de similaridade entre embeddings de todos os provedores.\n",
        "- Distribuição de similaridades (histograma) por provedor.\n",
        "- Gráfico de dispersão via PCA para embeddings combinados.\n",
        "\n",
        "**Interatividade**: selecione provedores e visualizações desejadas."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32dc902d",
      "metadata": {
        "id": "32dc902d"
      },
      "source": [
        "----\n",
        "## Seção 5a/14: Análise Exploratória & Visualização Avançada\n",
        "Aqui, exploraremos seu corpus com estatísticas e visualizações interativas:\n",
        "- Histogramas de comprimento de texto e distribuição de tokens.\n",
        "- Frequência de termos e bigramas.\n",
        "- Wordcloud interativa.\n",
        "- Estatísticas de entidades médicas usando spaCy.\n",
        "\n",
        "**Interatividade**: faça upload de CSV/TXT, ajuste top-k termos/bigramas e gere visualizações."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bece9575",
      "metadata": {
        "id": "bece9575"
      },
      "source": [
        "----\n",
        "## Seção 5b/14: Data Augmentation & Adversarial Tests\n",
        "Nesta subseção, você poderá:\n",
        "- Aplicar augmentations de texto clínico (back-translation, synonym replacement, random deletion).\n",
        "- Testar robustez com ataques adversariais via TextAttack.\n",
        "\n",
        "**Interatividade**: selecione métodos de augmentation/adversarial, configure parâmetros e visualize exemplos."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f374d9c",
      "metadata": {
        "id": "2f374d9c"
      },
      "source": [
        "----\n",
        "## Seção 6a/14: Fine‑Tuning Básico de Modelos\n",
        "Aqui, você poderá fazer o fine‑tuning de um modelo de seq2seq (T5/BART) usando seus dados clínicos:\n",
        "- Faça upload de `train.json` e `valid.json`.\n",
        "- Selecione o modelo base (`t5-small`, `bart-base`, etc.).\n",
        "- Defina hiperparâmetros: épocas, batch size, learning rate.\n",
        "- Acompanhe logs e salve o modelo fine‑tuned.\n",
        "\n",
        "**Interatividade**: carregue seus arquivos, configure e inicie o treinamento."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a460bfcb",
      "metadata": {
        "id": "a460bfcb"
      },
      "source": [
        "----\n",
        "## Seção 6b/14: Fine‑Tuning Avançado & Multi‑Task\n",
        "Nesta subseção, você poderá treinar um modelo em múltiplas tarefas simultaneamente:\n",
        "- **QA**: responder a perguntas baseadas em cenários clínicos.\n",
        "- **Resumo**: gerar resumos de relatos clínicos.\n",
        "- **Classificação**: classificar diagnósticos ou categorias.\n",
        "\n",
        "**Interatividade**: faça upload de um JSON multi-task, selecione tarefas, modelo e hiperparâmetros."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e7675a5",
      "metadata": {
        "id": "3e7675a5"
      },
      "source": [
        "----\n",
        "## Seção 7a/14: RAG Multimodal (Texto + Imagem)\n",
        "Nesta subseção, você construirá um sistema Retrieval-Augmented Generation que combina:\n",
        "- Embeddings de texto clínico (texto descritivo)\n",
        "- Embeddings de imagens médicas (radiografias, endoscopias) via CLIP\n",
        "- Indexação multimodal e recuperação conjunta\n",
        "- Geração de resposta via LLM com *chain-of-thought*\n",
        "\n",
        "**Interatividade**: faça upload de imagens e texto, configure modelos de embedding, ajuste top-k e veja a resposta multimodal."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50eebacb",
      "metadata": {
        "id": "50eebacb"
      },
      "source": [
        "----\n",
        "## Seção 7b/14: RAG Multimodal – Chain of Thought Detalhado\n",
        "Nesta subseção, vamos aprimorar a geração com **chain-of-thought**:\n",
        "- Permitir escolha de estilo de raciocínio (breve, detalhado).\n",
        "- Inserir pergunta clínica e exibir passo a passo do raciocínio.\n",
        "- Exibir fontes recuperadas (texto e imagens) como referências.\n",
        "\n",
        "**Interatividade**: insira sua pergunta, selecione o estilo e gere o raciocínio detalhado."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1313d87c",
      "metadata": {
        "id": "1313d87c"
      },
      "source": [
        "----\n",
        "## Seção 8a/14: Otimização de Hiperparâmetros Avançada\n",
        "Nesta subseção, você poderá:\n",
        "- Selecionar modelos treinados (Basic, Multi-Task, etc.).\n",
        "- Configurar espaços de busca para learning rate, batch size, weight decay e número de epochs.\n",
        "- Definir número de trials, estratégia de sampler (TPE, Random).\n",
        "- Integrar com W&B para monitorar experimentos.\n",
        "\n",
        "**Interatividade**: escolha o modelo, defina ranges e execute a busca.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e42de32",
      "metadata": {
        "id": "7e42de32"
      },
      "source": [
        "----\n",
        "## Seção 8b/14: Quantização & Pruning\n",
        "Nesta subseção, você poderá:\n",
        "- **Quantizar** o modelo (dinâmica INT8, estática INT8).\n",
        "- **Pruning** de magnitude (remoção de pesos irrelevantes).\n",
        "- Medir **tamanho** do modelo e **latência** de inferência.\n",
        "\n",
        "**Interatividade**: selecione o diretório do modelo, métodos e parâmetros de pruning."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39f06c3a",
      "metadata": {
        "id": "39f06c3a"
      },
      "source": [
        "----\n",
        "## Seção 9a/14: Containerização & API REST\n",
        "Nesta subseção, você poderá:\n",
        "- Gerar dinamicamente um **Dockerfile** para seu serviço.\n",
        "- Construir e executar o container localmente.\n",
        "- Testar endpoints `/predict`, `/health` e `/metrics`.\n",
        "\n",
        "**Interatividade**: configure `base_image`, `model_dir`, `service_file`, `porta` e `image_tag`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ffacbae4",
      "metadata": {
        "id": "ffacbae4"
      },
      "source": [
        "----\n",
        "## Seção 9b/14: CI/CD & Kubernetes\n",
        "Nesta subseção, vamos configurar:\n",
        "- **GitHub Actions** para CI/CD (build e push de imagem Docker)\n",
        "- **Helm Chart** básico para deploy em Kubernetes\n",
        "- Workflow interativo para ajustar nomes de repositório, tags e valores de Helm\n",
        "\n",
        "**Interatividade**: insira o nome do repositório, branch, Docker Registry, chart name e namespace."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2fe970d5",
      "metadata": {
        "id": "2fe970d5"
      },
      "source": [
        "----\n",
        "## Seção 10a/14: Monitoramento & Alertas\n",
        "Nesta subseção, você vai configurar:\n",
        "- **Prometheus** para coletar métricas da aplicação.\n",
        "- **Grafana** para dashboards interativos.\n",
        "- **Alertas no Slack** para thresholds (latência, erros).\n",
        "- **Relatórios programados** (e.g., uso de W&B ou cron) para envio de estatísticas.\n",
        "\n",
        "**Interatividade**: insira URL do Prometheus, dashboard ID do Grafana, webhook do Slack e thresholds para alertas."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9025fe8c",
      "metadata": {
        "id": "9025fe8c"
      },
      "source": [
        "----\n",
        "## Seção 10b/14: Relatórios Programados & Exportação\n",
        "Nesta subseção, você pode:\n",
        "- Gerar relatórios de métricas coletadas (Prometheus, W&B) em **HTML**, **PDF** ou **CSV**.\n",
        "- Customizar período de coleta (horas, dias).\n",
        "- Agendar relatórios diários/semanais automaticamente.\n",
        "\n",
        "**Interatividade**: selecione fonte de métricas, período, formato e agende o envio."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ccbd1482",
      "metadata": {
        "id": "ccbd1482"
      },
      "source": [
        "----\n",
        "## Seção 11a/14: Active Learning Avançado & Versionamento\n",
        "Nesta subseção, implementaremos:\n",
        "- Pipelines de **Active Learning** com estratégias avançadas (ensemble, adversarial sampling).\n",
        "- **Versionamento** automático de datasets e modelos usando DVC ou MLflow.\n",
        "- Interface para comparar versões anteriores e atuais de performance.\n",
        "\n",
        "**Interatividade**: selecione estratégia de sampling, configure versionamento e visualize histórico."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a770366",
      "metadata": {
        "id": "0a770366"
      },
      "source": [
        "----\n",
        "## Seção 11b/14: Comparação de Versões & Métricas\n",
        "Nesta subseção, você poderá:\n",
        "- Recuperar versões salvas (DVC/MLflow) e comparar métricas.\n",
        "- Visualizar métricas de avaliação (loss, accuracy) por versão.\n",
        "- Gerar gráficos interativos de evolução de métricas ao longo do tempo.\n",
        "\n",
        "**Interatividade**: selecione o experimento MLflow e as runs para comparar."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2a8931e",
      "metadata": {
        "id": "c2a8931e"
      },
      "source": [
        "----\n",
        "## Seção 12a/14: Construção Interativa de `train.json` e `valid.json`\n",
        "Nesta subseção, você poderá inserir casos clínicos diretamente no notebook para:\n",
        "- Escolher se o caso deve ir para **train** ou **valid**.\n",
        "- Descrever o cenário (ex.: \"Dor precordial há 47 min\").\n",
        "- Listar 3 diagnósticos diferenciais (DDx).\n",
        "- Indicar condutas (CD) e exames seguintes.\n",
        "- Perguntar e registrar métricas de acompanhamento (e.g., tempo de dor atual).\n",
        "\n",
        "**Interatividade**: preencha os campos abaixo, clique em **Adicionar Caso**, e veja os arquivos JSON atualizados."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "886799f2",
      "metadata": {
        "id": "886799f2"
      },
      "source": [
        "----\n",
        "## Seção 12b/14: Data Augmentation Avançada\n",
        "Nesta subseção, você poderá aplicar técnicas de **Data Augmentation** aos seus datasets `train.json` e `valid.json`:\n",
        "- Métodos: back-translation, synonym replacement, contextual substitution, random deletion/insertion.\n",
        "- Configure número de amostras geradas por exemplo.\n",
        "- Pré-visualize amostras e salve o dataset aumentado em `train_aug.json` ou `valid_aug.json`.\n",
        "\n",
        "**Interatividade**: selecione o dataset, métodos, parâmetros e execute a geração."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a49c952f",
      "metadata": {
        "id": "a49c952f"
      },
      "source": [
        "----\n",
        "## Seção 13a/14: Prompt Managers & Adequações\n",
        "Nesta subseção, você poderá:\n",
        "- Gerenciar diferentes **templates de prompt** para cenários médicos.\n",
        "- Ajustar dinamicamente **placeholders** e parâmetros (temperatura, max_tokens).\n",
        "- Salvar e recarregar **conjuntos de prompts** como JSON.\n",
        "- Testar rapidamente variações de prompt e comparar respostas.\n",
        "\n",
        "**Interatividade**: escolha ou crie um template, preencha valores e veja as respostas da LLM."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "711f550a",
      "metadata": {
        "id": "711f550a"
      },
      "source": [
        "----\n",
        "## Seção 13b/14: Testing OverTesting & Comparação Modelo-a-Modelo\n",
        "Nesta subseção, você poderá:\n",
        "- Executar testes de desempenho e robustez (overTesting) em diferentes LLMs.\n",
        "- Comparar respostas de múltiplos modelos para o mesmo conjunto de prompts.\n",
        "- Avaliar métricas como coerência, completude e similaridade semântica.\n",
        "\n",
        "**Interatividade**: selecione os modelos, faça upload de um arquivo JSON com prompts e compare resultados."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f3275cf",
      "metadata": {
        "id": "2f3275cf"
      },
      "source": [
        "----\n",
        "## Seção 14a/14: Revisão Conceitual Magistral\n",
        "Nesta subseção final, faremos uma revisão interativa dos principais conceitos abordados:\n",
        "- **Embeddings & Similaridade**\n",
        "- **Fine-Tuning & Multi-Task Learning**\n",
        "- **RAG (Retrieval-Augmented Generation)**\n",
        "- **Quantização & Pruning**\n",
        "- **Containerização & CI/CD**\n",
        "- **Monitoramento & Alertas**\n",
        "- **Active Learning & Versionamento**\n",
        "- **Prompt Management & Model-to-Model Testing**\n",
        "\n",
        "**Interatividade**: selecione um conceito para ver definição, exemplos e dicas de aplicação."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac32c393",
      "metadata": {
        "id": "ac32c393"
      },
      "source": [
        "----\n",
        "## Seção 14a2/14: Preparação para Exercícios de Alta Complexidade\n",
        "Antes de partir para exercícios desafiadores, responda às perguntas de aquecimento abaixo:\n",
        "- Questões de múltipla escolha revisando conceitos fundamentais\n",
        "- Feedback imediato e explicações para cada resposta\n",
        "\n",
        "**Interatividade**: selecione a opção correta e clique em Submeter para receber a explicação."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce47cd74",
      "metadata": {
        "id": "ce47cd74"
      },
      "source": [
        "----\n",
        "## Seção 14b/14: Hacks Secretos & Dicas Incríveis\n",
        "Explore truques avançados e otimizações para aprimorar seu fluxo:\n",
        "- **Memória de GPU extra**: ativar offloading de pesos para CPU.\n",
        "- **Pipeline de inferência assíncrona** para throughput máximo.\n",
        "- **Cache de embeddings compartilhado** entre modelos.\n",
        "- **Prompt templating avançado** com loops e condicionais.\n",
        "- **Uso de tokens especiais** para controlar o comportamento da LLM.\n",
        "\n",
        "**Interatividade**: selecione um hack para ver descrição e exemplo de código."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3f1a8f5",
      "metadata": {
        "id": "c3f1a8f5"
      },
      "source": [
        "----\n",
        "## Seção 14b2/14: Desafios de Código\n",
        "Teste suas habilidades com estes desafios práticos:\n",
        "1. **Implementar uma versão customizada de quantização dinâmica** sem usar ``torch.quantization``.\n",
        "2. **Criar uma função de caching de embeddings** que suporte expiração de entradas antigas.\n",
        "3. **Desenvolver um widget Jupyter** que compare em tempo real latências de múltiplos modelos.\n",
        "\n",
        "**Instruções:** selecione o desafio e clique em **Mostrar Desafio** para ver detalhes e estrutura de código inicial."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49370e27",
      "metadata": {
        "id": "49370e27"
      },
      "source": [
        "----\n",
        "## Seção 14b3/14: Gabarito dos Desafios de Código\n",
        "Aqui estão as soluções completas para os desafios propostos:\n",
        "1. **Quantização Customizada**\n",
        "2. **Cache de Embeddings com TTL**\n",
        "3. **Widget de Latência Interativo**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6facd28",
      "metadata": {
        "id": "b6facd28"
      },
      "source": [
        "----\n",
        "## Seção 14c/14: Ferramentas a Explorar para Melhorar\n",
        "Aqui estão algumas ferramentas e plataformas avançadas que você pode integrar para aprimorar seu workflow:\n",
        "- **Weights & Biases**: experiment tracking e visualização colaborativa.\n",
        "- **Haystack**: framework para pipelines RAG em produção.\n",
        "- **Streamlit / Gradio**: interfaces web interativas para demos.\n",
        "- **LangChain**: orquestração de LLMs e agentes.\n",
        "- **LlamaIndex (GPT Index)**: gerenciamento de índices de conhecimento.\n",
        "- **DeepSpeed**: treinamento e inferência em larga escala.\n",
        "- **Kedro**: orquestração de pipelines de dados.\n",
        "- **Apache Airflow**: agendamento e monitoramento de workflows.\n",
        "\n",
        "**Interatividade**: selecione uma ferramenta no dropdown para ver descrição e links."
      ]
    },
    {
      "source": [
        "!pip install --upgrade pip # Atualizar o pip\n",
        "!pip install --quiet \\\n",
        "    google-cloud-secret-manager==2.9.0 \\\n",
        "    google-cloud-storage==2.7.0 \\\n",
        "    transformers==4.28.1 \\\n",
        "    torch==2.0.0 \\\n",
        "    datasets==2.11.0 \\\n",
        "    faiss-cpu==1.7.4 \\\n",
        "    sentence-transformers==2.2.2 \\\n",
        "    python-multipart==0.0.5 \\\n",
        "    uvicorn==0.22.0 \\\n",
        "    fastapi==0.95.0 \\\n",
        "    typer==0.9.0 \\\n",
        "    click==8.3.2 \\\n",
        "    pydicom==2.3.1 \\\n",
        "    Pillow==9.5.0 \\\n",
        "    whisper==1.2.0 \\\n",
        "    spacy==3.5.3 \\\n",
        "    textattack==0.3.8 \\\n",
        "    optuna==3.2.0 \\\n",
        "    wandb==0.15.4 \\\n",
        "    haystack==1.17.0 \\\n",
        "    streamlit==1.23.1 \\\n",
        "    gradio==3.35.2 \\\n",
        "    langchain==2.0.0 \\\n",
        "    llama-index==0.6.12 \\\n",
        "    deepspeed==0.9.1 \\\n",
        "    kedro==2.0.3 \\\n",
        "    apache-airflow==2.6.3 \\\n",
        "    dvc==2.39.0 \\\n",
        "    mlflow==2.3.1 \\\n",
        "    # ... e outras bibliotecas relevantes para o futuro ..."
      ],
      "cell_type": "code",
      "metadata": {
        "id": "P6SqlZrVM3cZ"
      },
      "id": "P6SqlZrVM3cZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Justificativa para as Bibliotecas:\n",
        "\n",
        "Gerenciamento de pacotes:\n",
        "pip: Ferramenta essencial para gerenciar pacotes Python.\n",
        "Gerenciamento de segredos:\n",
        "google-cloud-secret-manager: Armazenar e acessar credenciais de forma segura.\n",
        "Armazenamento em nuvem:\n",
        "google-cloud-storage: Interagir com o Google Cloud Storage para armazenar e recuperar dados.\n",
        "Modelos de linguagem:\n",
        "transformers: Biblioteca para usar modelos de linguagem pré-treinados do Hugging Face.\n",
        "torch: Biblioteca de aprendizado de máquina para deep learning.\n",
        "sentence-transformers: Gerar embeddings de sentenças.\n",
        "Processamento de dados:\n",
        "datasets: Biblioteca para carregar e processar datasets do Hugging Face.\n",
        "faiss-cpu: Biblioteca para pesquisa de similaridade em embeddings.\n",
        "pydicom: Carregar e processar imagens médicas no formato DICOM.\n",
        "Pillow: Biblioteca para processamento de imagens.\n",
        "whisper: Transcrever áudio usando o modelo Whisper.\n",
        "spacy: Biblioteca para processamento de linguagem natural (NLP).\n",
        "Fine-tuning e otimização:\n",
        "textattack: Biblioteca para testes adversariais em modelos de linguagem.\n",
        "optuna: Biblioteca para otimização de hiperparâmetros.\n",
        "wandb: Biblioteca para rastreamento de experimentos e visualização de métricas.\n",
        "RAG e agentes:\n",
        "haystack: Framework para pipelines RAG (Retrieval-Augmented Generation).\n",
        "langchain: Orquestração de LLMs e agentes.\n",
        "llama-index: Gerenciamento de índices de conhecimento para RAG.\n",
        "Interfaces:\n",
        "streamlit: Criar interfaces web interativas para demos.\n",
        "gradio: Construir demos de ML com interfaces amigáveis e deploy simples.\n",
        "python-multipart: Suporte para upload de arquivos em APIs web.\n",
        "uvicorn: Servidor web ASGI para FastAPI.\n",
        "fastapi: Framework web para construir APIs REST.\n",
        "CLI:\n",
        "typer: Biblioteca para criar CLIs (Command-Line Interfaces) modernas.\n",
        "click: Biblioteca para criar CLIs simples.\n",
        "Orquestração e pipelines:\n",
        "deepspeed: Treinamento e inferência em larga escala.\n",
        "kedro: Orquestração de pipelines de dados.\n",
        "apache-airflow: Agendamento e monitoramento de workflows.\n",
        "Versionamento:\n",
        "dvc: Versionamento de dados e modelos.\n",
        "mlflow: Rastreamento de experimentos e gerenciamento de modelos.\n",
        "Observações:\n",
        "\n",
        "--quiet: O argumento --quiet no comando pip install suprime a saída detalhada da instalação. Remova-o se desejar visualizar o progresso da instalação.\n",
        "Versões: As versões das bibliotecas podem precisar ser ajustadas dependendo das suas necessidades e das dependências do projeto. Certifique-se de usar as versões compatíveis entre si.\n",
        "Bibliotecas adicionais: Inclua outras bibliotecas relevantes para as funcionalidades que você pretende adicionar ao projeto no futuro."
      ],
      "metadata": {
        "id": "0Nq3vcTaM-xi"
      },
      "id": "0Nq3vcTaM-xi"
    },
    {
      "source": [
        "!rm -rf my_env_colab\n",
        "!python3 -m venv my_env_colab"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3whHytQAPxK4",
        "outputId": "a18e9aa2-e38f-4eb6-cda7-f9000c6d1cd8"
      },
      "id": "3whHytQAPxK4",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: Command '['/content/my_env_colab/bin/python3', '-m', 'ensurepip', '--upgrade', '--default-pip']' returned non-zero exit status 1.\n"
          ]
        }
      ]
    },
    {
      "source": [
        "!source my_env_colab/bin/activate\n",
        "\n",
        "!my_env_colab/bin/python3 -m ensurepip --upgrade --default-pip\n",
        "!my_env_colab/bin/python3 -m pip install --upgrade pip\n",
        "!my_env_colab/bin/python3 -m pip install ipykernel\n",
        "!my_env_colab/bin/python3 -m ipykernel install --user --name my_env_colab --display-name \"Python (my_env)\""
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7DQeY4eN8dA",
        "outputId": "6b7544f5-d197-4418-8647-d9fc01ead3fc"
      },
      "id": "a7DQeY4eN8dA",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: Command '['/content/my_env_colab/bin/python3', '-m', 'ensurepip', '--upgrade', '--default-pip']' returned non-zero exit status 1.\n",
            "/bin/bash: line 1: my_env_colab/bin/activate: No such file or directory\n",
            "/content/my_env_colab/bin/python3: No module named ensurepip\n",
            "/content/my_env_colab/bin/python3: No module named pip\n",
            "/content/my_env_colab/bin/python3: No module named pip\n",
            "/content/my_env_colab/bin/python3: No module named ipykernel\n"
          ]
        }
      ]
    },
    {
      "source": [
        "!which python"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-c9PxOCQPb9b",
        "outputId": "cdc47503-0d8f-4514-ebc4-4f281f737193"
      },
      "id": "-c9PxOCQPb9b",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/bin/python\n"
          ]
        }
      ]
    },
    {
      "source": [
        "!pip install --upgrade pandas-gbq bigquery-magics bigframes google-auth-oauthlib google-cloud-bigquery google-cloud-pubsub google-cloud-resource-manager shapely\n",
        "!pip install --force-reinstall bigframes"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "LVxG4jVpN3R-"
      },
      "id": "LVxG4jVpN3R-",
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Instalar bibliotecas adicionais para o futuro\n",
        "!pip install --quiet \\\n",
        "    google-auth-oauthlib==0.5.3 \\\n",
        "    google-auth-httplib2==0.1.0 \\\n",
        "    google-api-python-client==2.84.0 \\\n",
        "    google-cloud-aiplatform==1.25.0 \\\n",
        "    google-cloud-bigquery==3.9.0 \\\n",
        "    google-cloud-logging==3.5.0 \\\n",
        "    google-cloud-pubsub==2.13.4 \\\n",
        "    google-cloud-translate==3.8.4 \\\n",
        "    # ... e outras bibliotecas relevantes para o futuro ...\n",
        "\n",
        "# Configurar a integração com o Google Cloud\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Definir o projeto padrão\n",
        "PROJECT_ID = \"MedicalScore\" # Substitua pelo ID real do seu projeto\n",
        "!gcloud config set project {PROJECT_ID}\n",
        "\n",
        "# Instalar o SDK do Google Cloud, se necessário\n",
        "!apt-get install -y google-cloud-sdk\n",
        "\n",
        "# Imprimir informações sobre a configuração\n",
        "!gcloud config list"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PbXSUZOiMwOE",
        "outputId": "923b741f-0893-4514-977d-2c4ddf76373b"
      },
      "id": "PbXSUZOiMwOE",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.1/43.1 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))': /simple/google-api-core/\u001b[0m\u001b[33m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m100.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m82.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m217.3/217.3 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.8/195.8 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m234.4/234.4 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.6/117.6 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.6/115.6 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.8/233.8 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pandas-gbq 0.28.0 requires google-auth-oauthlib>=0.7.0, but you have google-auth-oauthlib 0.5.3 which is incompatible.\n",
            "bigquery-magics 0.9.0 requires google-cloud-bigquery<4.0.0,>=3.13.0, but you have google-cloud-bigquery 3.9.0 which is incompatible.\n",
            "bigframes 2.1.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.9.0 which is incompatible.\n",
            "bigframes 2.1.0 requires google-cloud-pubsub>=2.21.4, but you have google-cloud-pubsub 2.13.4 which is incompatible.\n",
            "bigframes 2.1.0 requires google-cloud-resource-manager>=1.10.3, but you have google-cloud-resource-manager 1.6.3 which is incompatible.\n",
            "bigframes 2.1.0 requires shapely>=2.0.0, but you have shapely 1.8.5.post1 which is incompatible.\n",
            "google-cloud-iam 2.19.0 requires google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1, but you have google-api-core 2.10.2 which is incompatible.\n",
            "google-cloud-firestore 2.20.2 requires google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.0, but you have google-api-core 2.10.2 which is incompatible.\n",
            "tensorflow-metadata 1.17.1 requires protobuf<6.0.0,>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\n",
            "google-cloud-functions 1.20.3 requires google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1, but you have google-api-core 2.10.2 which is incompatible.\n",
            "google-cloud-bigquery-connection 1.18.2 requires google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1, but you have google-api-core 2.10.2 which is incompatible.\n",
            "google-cloud-spanner 3.54.0 requires google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.0, but you have google-api-core 2.10.2 which is incompatible.\n",
            "geopandas 1.0.1 requires shapely>=2.0.0, but you have shapely 1.8.5.post1 which is incompatible.\n",
            "google-cloud-bigquery-storage 2.31.0 requires google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.0, but you have google-api-core 2.10.2 which is incompatible.\n",
            "google-cloud-bigtable 2.30.1 requires google-api-core[grpc]<3.0.0,>=2.16.0, but you have google-api-core 2.10.2 which is incompatible.\n",
            "google-cloud-dataproc 5.18.1 requires google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1, but you have google-api-core 2.10.2 which is incompatible.\n",
            "google-ai-generativelanguage 0.6.15 requires google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1, but you have google-api-core 2.10.2 which is incompatible.\n",
            "google-cloud-datastore 2.21.0 requires google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.0, but you have google-api-core 2.10.2 which is incompatible.\n",
            "ydf 0.11.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\n",
            "dataproc-spark-connect 0.7.2 requires google-api-core>=2.19, but you have google-api-core 2.10.2 which is incompatible.\n",
            "google-cloud-language 2.17.1 requires google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1, but you have google-api-core 2.10.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[1;33mWARNING:\u001b[0m INVALID_ARGUMENT: Request contains an invalid argument.\n",
            "Are you sure you wish to set property [core/project] to MedicalScore?\n",
            "\n",
            "Do you want to continue (Y/n)?  y\n",
            "\n",
            "\u001b[1;31mERROR:\u001b[0m (gcloud.config.set) The project property must be set to a valid project ID, not the project name [MedicalScore]\n",
            "To set your project, run:\n",
            "\n",
            "  $ gcloud config set project PROJECT_ID\n",
            "\n",
            "or to unset it, run:\n",
            "\n",
            "  $ gcloud config unset project\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "E: Unable to locate package google-cloud-sdk\n",
            "[component_manager]\n",
            "disable_update_check = True\n",
            "[core]\n",
            "account = DrMatheusCoelho@gmail.com\n",
            "\n",
            "Your active configuration is: [default]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "75220713",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "75220713",
        "outputId": "c7e97084-b348-4167-ca5c-8d588d047255"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdrmatheuscoelho\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250509_102614-kutxvkkm</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/drmatheuscoelho/medical-llm-portfolio/runs/kutxvkkm' target=\"_blank\">polished-sea-1</a></strong> to <a href='https://wandb.ai/drmatheuscoelho/medical-llm-portfolio' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/drmatheuscoelho/medical-llm-portfolio' target=\"_blank\">https://wandb.ai/drmatheuscoelho/medical-llm-portfolio</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/drmatheuscoelho/medical-llm-portfolio/runs/kutxvkkm' target=\"_blank\">https://wandb.ai/drmatheuscoelho/medical-llm-portfolio/runs/kutxvkkm</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval_accuracy</td><td>▁</td></tr><tr><td>train_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval_accuracy</td><td>0.87</td></tr><tr><td>train_loss</td><td>0.42</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">polished-sea-1</strong> at: <a href='https://wandb.ai/drmatheuscoelho/medical-llm-portfolio/runs/kutxvkkm' target=\"_blank\">https://wandb.ai/drmatheuscoelho/medical-llm-portfolio/runs/kutxvkkm</a><br> View project at: <a href='https://wandb.ai/drmatheuscoelho/medical-llm-portfolio' target=\"_blank\">https://wandb.ai/drmatheuscoelho/medical-llm-portfolio</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250509_102614-kutxvkkm/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'haystack'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-23231ddc6828>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# 2. Haystack - Pipeline RAG Básico\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhaystack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocument_stores\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFAISSDocumentStore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhaystack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDensePassageRetriever\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFARMReader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRAGenerator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhaystack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipelines\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGenerativeQAPipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'haystack'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# Exemplos Robustos de Integração das Ferramentas da Seção 14c\n",
        "\n",
        "# 1. Weights & Biases - Configuração e Log\n",
        "import wandb\n",
        "wandb.init(project='medical-llm-portfolio', config={'learning_rate': 5e-5, 'batch_size': 8})\n",
        "wandb.log({'train_loss': 0.42, 'eval_accuracy': 0.87})\n",
        "wandb.finish()\n",
        "\n",
        "# 2. Haystack - Pipeline RAG Básico\n",
        "from haystack.document_stores import FAISSDocumentStore\n",
        "from haystack.nodes import DensePassageRetriever, FARMReader, RAGenerator\n",
        "from haystack.pipelines import GenerativeQAPipeline\n",
        "\n",
        "# Document store FAISS\n",
        "document_store = FAISSDocumentStore(faiss_index_factory_str=\"Flat\")\n",
        "retriever = DensePassageRetriever(document_store=document_store,\n",
        "                                  query_embedding_model=\"facebook/dpr-question_encoder-single-nq-base\",\n",
        "                                  passage_embedding_model=\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
        "document_store.update_embeddings(retriever)\n",
        "\n",
        "generator = RAGenerator(model_name_or_path=\"facebook/rag-token-base\")\n",
        "pipe = GenerativeQAPipeline(generator=generator, retriever=retriever)\n",
        "result = pipe.run(query=\"Quais são os sintomas de dissecção aórtica?\", params={\"Retriever\": {\"top_k\": 5}})\n",
        "print(result['answers'][0].answer)\n",
        "\n",
        "# 3. Streamlit - App Simples\n",
        "# salvar como app.py e rodar `streamlit run app.py`\n",
        "streamlit_code = '''\n",
        "import streamlit as st\n",
        "from transformers import pipeline\n",
        "\n",
        "st.title(\"Medical LLM Inference\")\n",
        "model = pipeline(\"text2text-generation\", model=\"google/flan-t5-small\")\n",
        "prompt = st.text_area(\"Prompt:\")\n",
        "if st.button(\"Gerar\"):\n",
        "    response = model(prompt, max_length=100)[0]['generated_text']\n",
        "    st.write(response)\n",
        "'''\n",
        "\n",
        "with open('app.py', 'w') as f:\n",
        "    f.write(streamlit_code)\n",
        "\n",
        "# 4. Gradio - Demo Interativo\n",
        "import gradio as gr\n",
        "def infer(prompt):\n",
        "    from transformers import pipeline\n",
        "    model = pipeline(\"text2text-generation\", model=\"google/flan-t5-small\")\n",
        "    return model(prompt, max_length=100)[0]['generated_text']\n",
        "\n",
        "demo = gr.Interface(fn=infer, inputs=\"text\", outputs=\"text\", title=\"Medical LLM Demo\")\n",
        "demo.launch(share=True)\n",
        "\n",
        "# 5. LangChain - Agente Pipeline\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "llm = OpenAI(model_name=\"gpt-3.5-turbo\")\n",
        "from langchain.vectorstores import FAISS\n",
        "faiss_store = FAISS.load_local(\"faiss_index\", emb_model=None)\n",
        "qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=faiss_store.as_retriever())\n",
        "print(qa_chain.run(\"Descreva a fisiologia renal.\"))\n",
        "\n",
        "# 6. LlamaIndex - Indexação e Consulta\n",
        "from llama_index import SimpleDirectoryReader, GPTVectorStoreIndex\n",
        "documents = SimpleDirectoryReader('docs/').load_data()\n",
        "index = GPTVectorStoreIndex.from_documents(documents)\n",
        "query_engine = index.as_query_engine()\n",
        "print(query_engine.query(\"Quais são os protocolos de analgesia pós-operatória?\"))\n",
        "\n",
        "# 7. DeepSpeed - Treinamento com ZeRO\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "import deepspeed\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
        "ds_config = {\n",
        "    \"train_micro_batch_size_per_gpu\": 4,\n",
        "    \"zero_optimization\": {\"stage\": 2}\n",
        "}\n",
        "model_engine, optimizer, _, _ = deepspeed.initialize(model=model, config=ds_config, model_parameters=model.parameters())\n",
        "# Exemplo de passo de treino\n",
        "inputs = tokenizer(\"Pacientes com febre\", return_tensors=\"pt\").to(model_engine.local_rank)\n",
        "outputs = model_engine(**inputs, labels=inputs.input_ids)\n",
        "loss = outputs.loss\n",
        "model_engine.backward(loss)\n",
        "model_engine.step()\n",
        "\n",
        "# 8. Apache Airflow - DAG Básico\n",
        "airflow_dag = '''\n",
        "from airflow import DAG\n",
        "from airflow.operators.python import PythonOperator\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "def train_model():\n",
        "    # código de treino\n",
        "    pass\n",
        "\n",
        "with DAG('train_and_deploy', start_date=datetime(2025,5,1), schedule_interval='@daily', catchup=False) as dag:\n",
        "    t1 = PythonOperator(task_id='train', python_callable=train_model)\n",
        "'''\n",
        "\n",
        "with open('dag.py', 'w') as f:\n",
        "    f.write(airflow_dag)\n",
        "print(\"Exemplos de código robustos gerados para todas as ferramentas da seção 14c.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "45d2533b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "45d2533b",
        "outputId": "67d59f5b-9465-48d0-a47e-125060c2b372"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax. Perhaps you forgot a comma? (<ipython-input-7-6b573eada946>, line 56)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-6b573eada946>\"\u001b[0;36m, line \u001b[0;32m56\u001b[0m\n\u001b[0;31m    [Site Oficial]({info['link']})\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
          ]
        }
      ],
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output, Markdown\n",
        "\n",
        "# Define tools with description and links\n",
        "tools = {\n",
        "    'Weights & Biases': {\n",
        "        'desc': 'Rastreie experimentos, visualize métricas e colabore em dashboards.',\n",
        "        'link': 'https://wandb.ai'\n",
        "    },\n",
        "    'Haystack': {\n",
        "        'desc': 'Construa pipelines RAG robustas para produção, com suporte a diversos backends.',\n",
        "        'link': 'https://haystack.deepset.ai'\n",
        "    },\n",
        "    'Streamlit': {\n",
        "        'desc': 'Crie aplicações web interativas de forma rápida com Python.',\n",
        "        'link': 'https://streamlit.io'\n",
        "    },\n",
        "    'Gradio': {\n",
        "        'desc': 'Construa demos de ML com interfaces amigáveis e deploy simples.',\n",
        "        'link': 'https://gradio.app'\n",
        "    },\n",
        "    'LangChain': {\n",
        "        'desc': 'Orquestre chamadas a LLMs, crie agentes e fluxos conversacionais.',\n",
        "        'link': 'https://python.langchain.com'\n",
        "    },\n",
        "    'LlamaIndex': {\n",
        "        'desc': 'Gerencie índices de conhecimento e realize buscas eficientes.',\n",
        "        'link': 'https://gpt-index.readthedocs.io'\n",
        "    },\n",
        "    'DeepSpeed': {\n",
        "        'desc': 'Acelere treinamento e inferência de modelos grandes com ZeRO e otimizações.',\n",
        "        'link': 'https://deepspeed.ai'\n",
        "    },\n",
        "    'Kedro': {\n",
        "        'desc': 'Orquestre pipelines de dados e ML com modularidade e reprodutibilidade.',\n",
        "        'link': 'https://kedro.apache.org'\n",
        "    },\n",
        "    'Apache Airflow': {\n",
        "        'desc': 'Agende, monitore e gerencie workflows complexos de dados.',\n",
        "        'link': 'https://airflow.apache.org'\n",
        "    }\n",
        "}\n",
        "\n",
        "tool_select = widgets.Dropdown(options=list(tools.keys()), description='Ferramenta:')\n",
        "out = widgets.Output()\n",
        "\n",
        "def show_tool(change):\n",
        "    with out:\n",
        "        clear_output()\n",
        "        t = change['new']\n",
        "        info = tools[t]\n",
        "        display(Markdown(f\"**{t}**\"))\n",
        "\n",
        "{info['desc']}\n",
        "\n",
        "[Site Oficial]({info['link']})\n",
        "\n",
        "tool_select.observe(show_tool, names='value')\n",
        "display(tool_select, out)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bermbriollmogy Portfolio v2\n",
        "\n",
        "## Descrição\n",
        "\n",
        "Este projeto implementa um pipeline completo para processamento multimodal de dados médicos e aplicação de modelos de linguagem (LLMs) em cenários clínicos, incluindo:\n",
        "\n",
        "* Ingestão de PDFs, imagens e áudios\n",
        "* Pré-processamento e desidentificação de texto\n",
        "* Geração de embeddings e similaridade multimodelo\n",
        "* Fine-tuning básico e multi-task\n",
        "* Retrieval-Augmented Generation (RAG) multimodal\n",
        "* Otimização (hyperparameter tuning, quantização, pruning)\n",
        "* Containerização e API REST\n",
        "* CI/CD automatizado e monitoramento\n",
        "* Active Learning avançado e versionamento\n",
        "* CLI para orquestração de tarefas\n",
        "\n",
        "## Sumário\n",
        "\n",
        "* [Estrutura do Projeto](#estrutura-do-projeto)\n",
        "* [Instalação](#instalação)\n",
        "* [Configuração](#configuração)\n",
        "* [Uso da CLI](#uso-da-cli)\n",
        "* [Testes](#testes)\n",
        "* [CI/CD](#cicd)\n",
        "* [Containerização](#containerização)\n",
        "* [Contribuição](#contribuição)\n",
        "* [Licença](#licença)\n",
        "\n",
        "---\n",
        "\n",
        "## Estrutura do Projeto\n",
        "\n",
        "```text\n",
        "bermbriollmogy_portfolio_v2/\n",
        "├── .github/\n",
        "│   └── workflows/\n",
        "│       └── ci.yml                  # Pipeline de CI (lint, testes, build & push Docker)\n",
        "├── config/\n",
        "│   ├── credentials.template.json   # Modelo de credenciais\n",
        "│   ├── logging.yaml                # Configuração de logging\n",
        "│   └── secret_manager.yaml         # Configurações Secret Manager\n",
        "├── docs/\n",
        "│   ├── architecture.md             # Arquitetura e diagramas\n",
        "│   ├── setup_guide.md              # Guia de instalação e configuração\n",
        "│   ├── usage.md                    # Exemplos de uso\n",
        "│   └── cli.md                      # Documentação da CLI\n",
        "├── notebooks/\n",
        "│   ├── bermbriollmogy_portfolio_v2.ipynb\n",
        "│   ├── quickstart.ipynb            # Teste “Olá Mundo” no LLM\n",
        "│   └── ingestion_demo.ipynb        # Demonstração de ingestão multimodal\n",
        "├── src/\n",
        "│   ├── __init__.py\n",
        "│   ├── main.py                     # Orquestra o pipeline completo\n",
        "│   ├── cli.py                      # CLI (Typer/Click)\n",
        "│   ├── utils/\n",
        "│   │   ├── credentials.py          # Carregamento seguro de credenciais\n",
        "│   │   ├── logger.py               # Setup de logs\n",
        "│   │   └── env_detector.py         # Detecta ambiente de execução\n",
        "│   ├── ingestion/                  # Módulo de ingestão multimodal\n",
        "│   ├── preprocessing/              # Limpeza, tokenização e dataset\n",
        "│   ├── embeddings/                 # Cálculo e comparação de embeddings\n",
        "│   ├── modeling/                   # Fine-tuning, RAG, otimização\n",
        "│   ├── deployment/                 # API REST, Docker, Helm chart\n",
        "│   ├── monitoring/                 # Prometheus, alertas\n",
        "│   └── active_learning/            # Estratégias e versionamento\n",
        "├── tests/\n",
        "│   ├── unit/                       # Testes unitários\n",
        "│   └── integration/                # Testes de integração\n",
        "├── scripts/\n",
        "│   ├── run_all.sh                  # Executa pipeline completo\n",
        "│   ├── build_image.sh              # Build & teste Docker\n",
        "│   └── deploy.sh                   # Deploy via Helm (staging/dev)\n",
        "├── .env.example                    # Exemplo de variáveis de ambiente\n",
        "├── pyproject.toml / poetry.lock    # Dependências (Poetry)\n",
        "├── requirements.txt (opcional)     # Lockfile alternativo\n",
        "├── README.md                       # Este arquivo\n",
        "└── LICENSE\n",
        "```\n",
        "\n",
        "## Instalação\n",
        "\n",
        "1. Clone o repositório:\n",
        "\n",
        "   ```bash\n",
        "   git clone https://github.com/seu-usuario/bermbriollmogy_portfolio_v2.git\n",
        "   cd bermbriollmogy_portfolio_v2\n",
        "   ```\n",
        "2. Instale as dependências via **Poetry** ou **pip**:\n",
        "\n",
        "   ```bash\n",
        "   # Poetry\n",
        "   poetry install\n",
        "\n",
        "   # Ou pip\n",
        "   pip install -r requirements.txt\n",
        "   ```\n",
        "\n",
        "## Configuração\n",
        "\n",
        "1. Copie e renomeie o template de credenciais:\n",
        "\n",
        "   ```bash\n",
        "   cp config/credentials.template.json config/credentials.json\n",
        "   ```\n",
        "2. Preencha suas chaves e tokens no `config/credentials.json` **ou** use variáveis de ambiente em `.env`:\n",
        "\n",
        "   ```dotenv\n",
        "   OPENAI_API_KEY=sk-...\n",
        "   HF_TOKEN=hf_...\n",
        "   GEMINI_API_KEY=...\n",
        "   SLACK_WEBHOOK_URL=https://hooks.slack.com/...\n",
        "   ```\n",
        "3. (Opcional) Configure CLI em `~/.berm/config.toml`:\n",
        "\n",
        "   ```toml\n",
        "   [defaults]\n",
        "   source = \"./data\"\n",
        "   format = \"pdf\"\n",
        "   workers = 4\n",
        "   ```\n",
        "\n",
        "## Uso da CLI\n",
        "\n",
        "Após a instalação, a CLI (`berm`) permite orquestrar o pipeline:\n",
        "\n",
        "* **Pipeline completo**:\n",
        "\n",
        "  ```bash\n",
        "  berm run\n",
        "  ```\n",
        "* **Ingestão**:\n",
        "\n",
        "  ```bash\n",
        "  berm ingest --source ./pdfs --format pdf --out-dir ./chunks\n",
        "  ```\n",
        "* **Pré-processamento**:\n",
        "\n",
        "  ```bash\n",
        "  berm preprocess --input-dir ./chunks --cache-dir ./cache --workers 4\n",
        "  ```\n",
        "* **Embeddings**:\n",
        "\n",
        "  ```bash\n",
        "  berm embed --input ./processed.json --provider openai\n",
        "  ```\n",
        "* **Fine-tuning**:\n",
        "\n",
        "  ```bash\n",
        "  berm finetune --model t5-small --epochs 3 --batch-size 8\n",
        "  ```\n",
        "* **Deploy**:\n",
        "\n",
        "  ```bash\n",
        "  berm deploy --env dev\n",
        "  ```\n",
        "\n",
        "Para ver todas as opções de cada comando:\n",
        "\n",
        "```bash\n",
        "berm <comando> --help\n",
        "```\n",
        "\n",
        "## Testes\n",
        "\n",
        "* **Unitários**:\n",
        "\n",
        "  ```bash\n",
        "  pytest tests/unit\n",
        "  ```\n",
        "* **Integração**:\n",
        "\n",
        "  ```bash\n",
        "  pytest tests/integration\n",
        "  ```\n",
        "* **Cobertura**:\n",
        "\n",
        "  ```bash\n",
        "  pytest --cov=src\n",
        "  ```\n",
        "\n",
        "## CI/CD\n",
        "\n",
        "O pipeline GitHub Actions (`.github/workflows/ci.yml`) realiza:\n",
        "\n",
        "* Lint (`flake8`)\n",
        "* Testes (unit & integration) em múltiplas versões de Python\n",
        "* Build da imagem Docker\n",
        "* Push automático para Docker Hub na branch `main`\n",
        "\n",
        "## Containerização\n",
        "\n",
        "Build e run:\n",
        "\n",
        "```bash\n",
        "# Build image\n",
        "docker build -f src/deployment/Dockerfile -t bermbriollmogy:latest .\n",
        "\n",
        "# Run container\n",
        "docker run --env-file .env -p 8000:8000 bermbriollmogy:latest\n",
        "```\n",
        "\n",
        "## Contribuição\n",
        "\n",
        "1. Abra uma *issue* descrevendo sua sugestão.\n",
        "2. Fork este repositório.\n",
        "3. Crie uma *branch* `feature/nova-funcionalidade`.\n",
        "4. Faça commits claros e atômicos.\n",
        "5. Abra um *pull request* para `develop`.\n",
        "\n",
        "## Licença\n",
        "\n",
        "Este projeto está licenciado sob a [MIT License](LICENSE).\n"
      ],
      "metadata": {
        "id": "ITFQPxfQr3V2"
      },
      "id": "ITFQPxfQr3V2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bermbriollmogy Portfolio v2 – Avançado\n",
        "\n",
        "[![CI Status](https://img.shields.io/github/actions/workflow/status/SEU_USUARIO/bermbriollmogy_portfolio_v2/ci.yml?branch=main)](https://github.com/SEU_USUARIO/bermbriollmogy_portfolio_v2/actions)  \n",
        "[![Coverage Status](https://img.shields.io/codecov/c/github/SEU_USUARIO/bermbriollmogy_portfolio_v2/main)](https://codecov.io/gh/SEU_USUARIO/bermbriollmogy_portfolio_v2)  \n",
        "[![License](https://img.shields.io/github/license/SEU_USUARIO/bermbriollmogy_portfolio_v2)](./LICENSE)\n",
        "\n",
        "---\n",
        "\n",
        "## 📖 Sumário\n",
        "\n",
        "- [Sobre](#-sobre)  \n",
        "- [Diagrama de Arquitetura](#-diagrama-de-arquitetura)  \n",
        "- [Funcionalidades](#-funcionalidades)  \n",
        "- [Badges](#-badges)  \n",
        "- [Instalação](#-instalação)  \n",
        "- [CLI](#-cli)  \n",
        "- [Testes](#-testes)  \n",
        "- [CI/CD](#-cicd)  \n",
        "- [Containerização](#-containerização)  \n",
        "- [Documentação e Recursos](#-documentação-e-recursos)  \n",
        "- [Demonstrações](#-demonstrações)  \n",
        "- [Contribuição](#-contribuição)  \n",
        "- [Licença](#-licença)  \n",
        "- [Contato & CTA](#-contato--cta)  \n",
        "\n",
        "---\n",
        "\n",
        "## 📄 Sobre\n",
        "\n",
        "Este projeto oferece um pipeline completo, modular e escalável para ingestão multimodal, pré-processamento, fine-tuning de LLMs, RAG multimodal, deploy em container com API REST, monitoramento e active learning em cenários clínicos.\n",
        "\n",
        "---\n",
        "\n",
        "## 🏛️ Diagrama de Arquitetura\n",
        "\n",
        "![Arquitetura Geral](docs/images/architecture.png)\n",
        "\n",
        "> **Figura:** Fluxo completo, da ingestão à inferência em produção.\n",
        "\n",
        "---\n",
        "\n",
        "## ✨ Funcionalidades\n",
        "\n",
        "- **Ingestão multimodal**: PDFs, imagens DICOM/PNG, áudio (Whisper), FHIR, web scraping.  \n",
        "- **Pré-processamento**: limpeza, desidentificação, tokenização paralela, cache.  \n",
        "- **Embeddings & Similaridade**: múltiplos provedores (OpenAI, HF, Vertex AI).  \n",
        "- **Fine-tuning e Multi-Task**: T5/BART, QA, resumo, classificação.  \n",
        "- **RAG Multimodal**: texto + imagem + Chain-of-Thought.  \n",
        "- **Otimização**: quantização, pruning, hyperopt (W&B).  \n",
        "- **Deploy**: FastAPI/Flask, Docker multi-stage, Kubernetes Helm.  \n",
        "- **Monitoramento**: Prometheus, Grafana, alertas Slack/e-mail.  \n",
        "- **Active Learning & Versionamento**: DVC/MLflow, sampling avançado.  \n",
        "- **CLI**: `berm` — orquestra todo o pipeline.  \n",
        "\n",
        "---\n",
        "\n",
        "## 🏷️ Badges\n",
        "\n",
        "- **CI Status**  \n",
        "  ![CI Status](https://img.shields.io/github/actions/workflow/status/SEU_USUARIO/bermbriollmogy_portfolio_v2/ci.yml?branch=main)  \n",
        "- **Coverage**  \n",
        "  ![Coverage](https://img.shields.io/codecov/c/github/SEU_USUARIO/bermbriollmogy_portfolio_v2/main)  \n",
        "- **License**  \n",
        "  ![License](https://img.shields.io/github/license/SEU_USUARIO/bermbriollmogy_portfolio_v2)  \n",
        "\n",
        "(adapte URLs para o seu repositório)\n",
        "\n",
        "---\n",
        "\n",
        "## 💾 Instalação\n",
        "\n",
        "1. **Clone**  \n",
        "   ```bash\n",
        "   git clone https://github.com/SEU_USUARIO/bermbriollmogy_portfolio_v2.git\n",
        "   cd bermbriollmogy_portfolio_v2\n",
        "\n",
        "\t2.\tDependências\n",
        "\n",
        "pip install poetry\n",
        "poetry install\n",
        "\n",
        "\n",
        "\t3.\tConfiguração\n",
        "\t•\tCopie config/credentials.template.json → config/credentials.json e preencha suas chaves.\n",
        "\t•\tCopie .env.example → .env e ajuste variáveis de ambiente (OPENAI_API_KEY, etc.).\n",
        "\n",
        "⸻\n",
        "\n",
        "💻 CLI\n",
        "\n",
        "Após instalação, o comando principal é berm.\n",
        "\n",
        "# pipeline completo\n",
        "berm run\n",
        "\n",
        "# apenas ingestão multimodal\n",
        "berm ingest --source ./data/pdfs --format pdf\n",
        "\n",
        "# pré-processamento com cache\n",
        "berm preprocess --input-dir ./chunks --cache-dir ./cache --workers 4\n",
        "\n",
        "Para ver todos os subcomandos e opções:\n",
        "\n",
        "berm --help\n",
        "berm ingest --help\n",
        "\n",
        "Consulte docs/cli.md para documentação completa.\n",
        "\n",
        "⸻\n",
        "\n",
        "🧪 Testes\n",
        "\t•\tUnitários:\n",
        "\n",
        "pytest tests/unit\n",
        "\n",
        "\n",
        "\t•\tIntegração:\n",
        "\n",
        "pytest tests/integration\n",
        "\n",
        "\n",
        "\n",
        "Cobertura garantida via Codecov.\n",
        "Configure pytest.ini e tox.ini conforme seu fluxo de trabalho, se desejar.\n",
        "\n",
        "⸻\n",
        "\n",
        "🚀 CI/CD\n",
        "\n",
        "Automatizado via GitHub Actions em .github/workflows/ci.yml:\n",
        "\t•\tJobs\n",
        "\t•\tbuild: lint (flake8), instalação via Poetry, testes.\n",
        "\t•\tdocker: build & push de imagem Docker ao main.\n",
        "\n",
        "Adapte secrets (DOCKER_USERNAME, DOCKER_PASSWORD) em Settings → Secrets.\n",
        "\n",
        "⸻\n",
        "\n",
        "🐳 Containerização\n",
        "\n",
        "O src/deployment/Dockerfile utiliza multi-stage build para deixar a imagem enxuta:\n",
        "\n",
        "# builder\n",
        "FROM python:3.10-slim AS builder\n",
        "...\n",
        "# runtime\n",
        "FROM python:3.10-slim\n",
        "...\n",
        "CMD [\"uvicorn\", \"src.deployment.api:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
        "\n",
        "Para testar localmente:\n",
        "\n",
        "docker build -t bermbriollmogy:latest -f src/deployment/Dockerfile .\n",
        "docker run -p 8000:8000 bermbriollmogy:latest\n",
        "\n",
        "\n",
        "⸻\n",
        "\n",
        "📚 Documentação e Recursos\n",
        "\t•\tGuia de Setup: docs/setup_guide.md\n",
        "\t•\tUso & Exemplos: docs/usage.md\n",
        "\t•\tArquitetura: docs/architecture.md\n",
        "\t•\tTutoriais:\n",
        "\t•\tIntegração com GCP Secret Manager\n",
        "\t•\tRAG multimodal com LangChain\n",
        "\t•\tMonitoramento avançado com Grafana\n",
        "\n",
        "⸻\n",
        "\n",
        "🔗 Demonstrações\n",
        "\t•\tColab Demo: ▶️ Executar no Colab\n",
        "\t•\tStreamlit App: 🔗 Acesse a demo\n",
        "\n",
        "⸻\n",
        "\n",
        "🤝 Contribuição\n",
        "\n",
        "Contribuições são bem-vindas!\n",
        "\t1.\tFork e branch (feature/xyz).\n",
        "\t2.\tCommit com convenção de mensagens.\n",
        "\t3.\tPR com descrição clara e updates no CHANGELOG.md.\n",
        "\n",
        "Veja o CONTRIBUTING.md para diretrizes completas.\n",
        "\n",
        "⸻\n",
        "\n",
        "📝 Licença\n",
        "\n",
        "Este projeto está licenciado sob a MIT License.\n",
        "\n",
        "⸻\n",
        "\n",
        "📞 Contato & Call to Action\n",
        "\n",
        "Gostou do projeto? Experimente agora:\n",
        "\n",
        "git clone https://github.com/SEU_USUARIO/bermbriollmogy_portfolio_v2.git\n",
        "cd bermbriollmogy_portfolio_v2\n",
        "poetry install\n",
        "berm run --dry-run\n",
        "\n",
        "👉 Participe: abra issues, envie PRs, compartilhe feedback!\n",
        "👉 Fale conosco: projetoml@seudominio.com.br\n",
        "\n",
        "⸻\n",
        "\n",
        "Obrigado por usar o Bermbriollmogy Portfolio v2! 🚀"
      ],
      "metadata": {
        "id": "M_fp3QdtsnkB"
      },
      "id": "M_fp3QdtsnkB"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bermbriollmogy Portfolio v2 – Avançado\n",
        "\n",
        "[![CI Status](https://img.shields.io/github/actions/workflow/status/SEU_USUARIO/bermbriollmogy_portfolio_v2/ci.yml?branch=main)](https://github.com/SEU_USUARIO/bermbriollmogy_portfolio_v2/actions)  \n",
        "[![Coverage Status](https://img.shields.io/codecov/c/github/SEU_USUARIO/bermbriollmogy_portfolio_v2/main)](https://codecov.io/gh/SEU_USUARIO/bermbriollmogy_portfolio_v2)  \n",
        "[![License](https://img.shields.io/github/license/SEU_USUARIO/bermbriollmogy_portfolio_v2)](./LICENSE)\n",
        "\n",
        "---\n",
        "\n",
        "## 📖 Sumário\n",
        "\n",
        "- [Sobre](#-sobre)  \n",
        "- [Diagrama de Arquitetura](#-diagrama-de-arquitetura)  \n",
        "- [Demonstração em GIF](#-demonstração-em-gif)  \n",
        "- [Funcionalidades](#-funcionalidades)  \n",
        "- [Badges](#-badges)  \n",
        "- [Instalação](#-instalação)  \n",
        "- [CLI](#-cli)  \n",
        "- [Testes](#-testes)  \n",
        "- [CI/CD](#-cicd)  \n",
        "- [Containerização](#-containerização)  \n",
        "- [Documentação e Recursos](#-documentação-e-recursos)  \n",
        "- [Demonstrações](#-demonstrações)  \n",
        "- [Roadmap](#-roadmap)  \n",
        "- [Comunidade & Chat](#-comunidade--chat)  \n",
        "- [Contribuição](#-contribuição)  \n",
        "- [Licença](#-licença)  \n",
        "- [Contato & CTA](#-contato--cta)  \n",
        "\n",
        "---\n",
        "\n",
        "## 📄 Sobre\n",
        "\n",
        "Este projeto oferece um pipeline completo, modular e escalável para ingestão multimodal, pré-processamento, fine-tuning de LLMs, RAG multimodal, deploy em container com API REST, monitoramento e active learning em cenários clínicos.\n",
        "\n",
        "---\n",
        "\n",
        "## 🏛️ Diagrama de Arquitetura\n",
        "\n",
        "![Arquitetura Geral](docs/images/architecture.png)\n",
        "\n",
        "> **Figura:** Fluxo completo, da ingestão à inferência em produção.\n",
        "\n",
        "---\n",
        "\n",
        "## 🎬 Demonstração em GIF\n",
        "\n",
        "![Demonstração Rápida](docs/images/demo.gif)\n",
        "\n",
        "> **GIF:** Exemplo de uso do `berm run` e resultados no notebook.\n",
        "\n",
        "---\n",
        "\n",
        "## ✨ Funcionalidades\n",
        "\n",
        "- **Ingestão multimodal**: PDFs, imagens DICOM/PNG, áudio (Whisper), FHIR, web scraping.  \n",
        "- **Pré-processamento**: limpeza, desidentificação, tokenização paralela, cache.  \n",
        "- **Embeddings & Similaridade**: múltiplos provedores (OpenAI, HF, Vertex AI).  \n",
        "- **Fine-tuning e Multi-Task**: T5/BART, QA, resumo, classificação.  \n",
        "- **RAG Multimodal**: texto + imagem + Chain-of-Thought.  \n",
        "- **Otimização**: quantização, pruning, hyperopt (W&B).  \n",
        "- **Deploy**: FastAPI/Flask, Docker multi-stage, Kubernetes Helm.  \n",
        "- **Monitoramento**: Prometheus, Grafana, alertas Slack/e-mail.  \n",
        "- **Active Learning & Versionamento**: DVC/MLflow, sampling avançado.  \n",
        "- **CLI**: `berm` — orquestra todo o pipeline.  \n",
        "\n",
        "---\n",
        "\n",
        "## 🏷️ Badges\n",
        "\n",
        "- **CI Status**  \n",
        "  ![CI Status](https://img.shields.io/github/actions/workflow/status/SEU_USUARIO/bermbriollmogy_portfolio_v2/ci.yml?branch=main)  \n",
        "- **Coverage**  \n",
        "  ![Coverage](https://img.shields.io/codecov/c/github/SEU_USUARIO/bermbriollmogy_portfolio_v2/main)  \n",
        "- **License**  \n",
        "  ![License](https://img.shields.io/github/license/SEU_USUARIO/bermbriollmogy_portfolio_v2)  \n",
        "\n",
        "(adapte URLs para o seu repositório)\n",
        "\n",
        "---\n",
        "\n",
        "## 💾 Instalação\n",
        "\n",
        "1. **Clone**  \n",
        "   ```bash\n",
        "   git clone https://github.com/SEU_USUARIO/bermbriollmogy_portfolio_v2.git\n",
        "   cd bermbriollmogy_portfolio_v2\n",
        "\n",
        "\t2.\tDependências\n",
        "\n",
        "pip install poetry\n",
        "poetry install\n",
        "\n",
        "\n",
        "\t3.\tConfiguração\n",
        "\t•\tCopie config/credentials.template.json → config/credentials.json e preencha suas chaves.\n",
        "\t•\tCopie .env.example → .env e ajuste variáveis de ambiente (OPENAI_API_KEY, etc.).\n",
        "\n",
        "⸻\n",
        "\n",
        "💻 CLI\n",
        "\n",
        "Após instalação, o comando principal é berm.\n",
        "\n",
        "# pipeline completo\n",
        "berm run\n",
        "\n",
        "# apenas ingestão multimodal\n",
        "berm ingest --source ./data/pdfs --format pdf\n",
        "\n",
        "# pré-processamento com cache\n",
        "berm preprocess --input-dir ./chunks --cache-dir ./cache --workers 4\n",
        "\n",
        "Para ver todos os subcomandos e opções:\n",
        "\n",
        "berm --help\n",
        "berm ingest --help\n",
        "\n",
        "Consulte docs/cli.md para documentação completa.\n",
        "\n",
        "⸻\n",
        "\n",
        "🧪 Testes\n",
        "\t•\tUnitários:\n",
        "\n",
        "pytest tests/unit\n",
        "\n",
        "\n",
        "\t•\tIntegração:\n",
        "\n",
        "pytest tests/integration\n",
        "\n",
        "\n",
        "\n",
        "Cobertura garantida via Codecov.\n",
        "\n",
        "⸻\n",
        "\n",
        "🚀 CI/CD\n",
        "\n",
        "Automatizado via GitHub Actions em .github/workflows/ci.yml:\n",
        "\t•\tJobs\n",
        "\t•\tbuild: lint (flake8), instalação via Poetry, testes.\n",
        "\t•\tdocker: build & push de imagem Docker ao main.\n",
        "\n",
        "Adapte secrets (DOCKER_USERNAME, DOCKER_PASSWORD) em Settings → Secrets.\n",
        "\n",
        "⸻\n",
        "\n",
        "🐳 Containerização\n",
        "\n",
        "O src/deployment/Dockerfile utiliza multi-stage build para deixar a imagem enxuta:\n",
        "\n",
        "# builder\n",
        "FROM python:3.10-slim AS builder\n",
        "...\n",
        "# runtime\n",
        "FROM python:3.10-slim\n",
        "...\n",
        "CMD [\"uvicorn\", \"src.deployment.api:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
        "\n",
        "Para testar localmente:\n",
        "\n",
        "docker build -t bermbriollmogy:latest -f src/deployment/Dockerfile .\n",
        "docker run -p 8000:8000 bermbriollmogy:latest\n",
        "\n",
        "\n",
        "⸻\n",
        "\n",
        "📚 Documentação e Recursos\n",
        "\t•\tGuia de Setup: docs/setup_guide.md\n",
        "\t•\tUso & Exemplos: docs/usage.md\n",
        "\t•\tArquitetura: docs/architecture.md\n",
        "\t•\tTutoriais:\n",
        "\t•\tIntegração com GCP Secret Manager\n",
        "\t•\tRAG multimodal com LangChain\n",
        "\t•\tMonitoramento avançado com Grafana\n",
        "\n",
        "⸻\n",
        "\n",
        "🔗 Demonstrações\n",
        "\t•\tColab Demo: ▶️ Executar no Colab\n",
        "\t•\tStreamlit App: 🔗 Acesse a demo\n",
        "\n",
        "⸻\n",
        "\n",
        "🗺️ Roadmap\n",
        "\n",
        "Versão\tDescrição\tData Estimada\n",
        "v2.1\tIntegração com LangChain Agents\tQ3 2025\n",
        "v2.2\tSuporte a banco de dados vetorial local\tQ4 2025\n",
        "v3.0\tUI web full-featured (React + FastAPI)\tQ1 2026\n",
        "v3.1\tMobile SDK (iOS & Android)\tQ2 2026\n",
        "\n",
        "\n",
        "⸻\n",
        "\n",
        "💬 Comunidade & Chat\n",
        "\n",
        "Junte-se à nossa comunidade para discutir, tirar dúvidas e compartilhar ideias:\n",
        "\t•\tDiscord: https://discord.gg/seu-invite\n",
        "\t•\tSlack: https://join.slack.com/t/seu-workspace/shared_invite/xyz\n",
        "\n",
        "⸻\n",
        "\n",
        "🤝 Contribuição\n",
        "\n",
        "Contribuições são bem-vindas!\n",
        "\t1.\tFork e branch (feature/xyz).\n",
        "\t2.\tCommit com convenção de mensagens.\n",
        "\t3.\tPR com descrição clara e updates no CHANGELOG.md.\n",
        "\n",
        "Veja o CONTRIBUTING.md para diretrizes completas.\n",
        "\n",
        "⸻\n",
        "\n",
        "📝 Licença\n",
        "\n",
        "Este projeto está licenciado sob a MIT License.\n",
        "\n",
        "⸻\n",
        "\n",
        "📞 Contato & Call to Action\n",
        "\n",
        "Gostou do projeto? Experimente agora:\n",
        "\n",
        "git clone https://github.com/SEU_USUARIO/bermbriollmogy_portfolio_v2.git\n",
        "cd bermbriollmogy_portfolio_v2\n",
        "poetry install\n",
        "berm run --dry-run\n",
        "\n",
        "👉 Participe: abra issues, envie PRs, compartilhe feedback!\n",
        "👉 Fale conosco: projetoml@seudominio.com.br\n",
        "\n",
        "⸻\n",
        "\n",
        "Obrigado por usar o Bermbriollmogy Portfolio v2! 🚀\n",
        "\n"
      ],
      "metadata": {
        "id": "OrrBSqjktI80"
      },
      "id": "OrrBSqjktI80"
    },
    {
      "cell_type": "markdown",
      "id": "d3bd9386",
      "metadata": {
        "id": "d3bd9386"
      },
      "source": [
        "----\n",
        "## Seção 14d/14: Continuação & Próximos Passos\n",
        "Esta é a conclusão do portfolio. A seguir, suas opções para continuar evoluindo:\n",
        "1. **Projetos de Produção**: deploy em ambiente Kubernetes gerenciado, pipelines Airflow.\n",
        "2. **Custom Agents**: use LangChain Agents para fluxos complexos (LlamaIndex, Tools).\n",
        "3. **Pesquisa Avançada**: explore papers recentes em ArXiv sobre fine-tuning e RAG.\n",
        "4. **Contribuição Open Source**: participe de projetos como Haystack, Transformers.\n",
        "5. **Comunidade & Eventos**: compareça a conferências (NeurIPS, ACL) e meetups locais.\n",
        "6. **Publicação**: escreva artigos ou blogs sobre seu pipeline médico-LLM.\n",
        "\n",
        "**Interatividade**: selecione um próximo passo para ver detalhes, sugestões de links e comandos iniciais."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f673457b",
      "metadata": {
        "id": "f673457b"
      },
      "outputs": [],
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output, Markdown\n",
        "\n",
        "steps = {\n",
        "    'Produção em Kubernetes': (\n",
        "        'Use Helm e GitOps para deploy contínuo. Cmd ex.: helm upgrade --install ...',\n",
        "        ['https://kubernetes.io', 'https://helm.sh']\n",
        "    ),\n",
        "    'Custom Agents': (\n",
        "        'Defina ferramentas e memória para agentes LangChain. Inicie com: from langchain.agents import initialize_agent',\n",
        "        ['https://python.langchain.com/docs/get_started/agents']\n",
        "    ),\n",
        "    'Pesquisa Avançada': (\n",
        "        'Leia artigos recentes sobre RAG e RLHF. Exemplo: https://arxiv.org/abs/2208.07442',\n",
        "        ['https://arxiv.org/search/?query=retrieval+augmented']\n",
        "    ),\n",
        "    'Contribuição Open Source': (\n",
        "        'Fork projetos, abra issues e PRs. Exemplo: haystack deepset/haystack no GitHub.',\n",
        "        ['https://github.com/deepset-ai/haystack']\n",
        "    ),\n",
        "    'Comunidade & Eventos': (\n",
        "        'Participe de NeurIPS, ACL, RLDM. Verifique CFPs e deadlines.',\n",
        "        ['https://neurips.cc', 'https://aclanthology.org']\n",
        "    ),\n",
        "    'Publicação': (\n",
        "        'Templates JupyterBook e Sphinx para documentación. Inicie: jupyter-book create mybook/',\n",
        "        ['https://jupyter.org/jupyter-book']\n",
        "    )\n",
        "}\n",
        "\n",
        "dropdown = widgets.Dropdown(options=list(steps.keys()), description='Próximo Passo:')\n",
        "output = widgets.Output()\n",
        "\n",
        "def show_step(change):\n",
        "    with output:\n",
        "        clear_output()\n",
        "        key = change['new']\n",
        "        desc, links = steps[key]\n",
        "        display(Markdown(f\"### {key}\n",
        "\n",
        "{desc}\n",
        "\n",
        "Links úteis:\"))\n",
        "        for link in links:\n",
        "            display(Markdown(f\"- [{link}]({link})\"))\n",
        "\n",
        "dropdown.observe(show_step, names='value')\n",
        "display(dropdown, output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ceb46c9d",
      "metadata": {
        "id": "ceb46c9d"
      },
      "outputs": [],
      "source": [
        "# 1. Quantização Customizada\n",
        "import torch\n",
        "\n",
        "def dynamic_quantize_custom(model):\n",
        "    # Cria uma cópia do modelo para não sobrescrever o original\n",
        "    quantized_model = model.__class__.from_pretrained(model.config._name_or_path)\n",
        "    quantized_model.load_state_dict(model.state_dict())\n",
        "    # Converte pesos de cada Linear para INT8\n",
        "    for name, module in quantized_model.named_modules():\n",
        "        if isinstance(module, torch.nn.Linear):\n",
        "            # Obtém os pesos originais\n",
        "            weight = module.weight.data\n",
        "            # Calcula escala e zero_point\n",
        "            min_val, max_val = weight.min(), weight.max()\n",
        "            qmin, qmax = -128, 127\n",
        "            scale = (max_val - min_val) / (qmax - qmin)\n",
        "            zero_point = (qmax + qmin) / 2 - max_val / scale\n",
        "            # Quantiza\n",
        "            q_weight = ((weight / scale) + zero_point).round().clamp(qmin, qmax).char()\n",
        "            # Dequantiza para manter o tipo float (opcional)\n",
        "            module.weight.data = ((q_weight.float() - zero_point) * scale)\n",
        "    return quantized_model\n",
        "\n",
        "# Exemplo de uso\n",
        "# quant_model = dynamic_quantize_custom(model)\n",
        "# quant_model.generate(input_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "104873c3",
      "metadata": {
        "id": "104873c3"
      },
      "outputs": [],
      "source": [
        "# 2. Cache de Embeddings com TTL\n",
        "import time\n",
        "\n",
        "class EmbeddingCache:\n",
        "    def __init__(self, ttl_seconds):\n",
        "        self.ttl = ttl_seconds\n",
        "        self.cache = {}  # chave: texto, valor: (embedding, timestamp)\n",
        "\n",
        "    def get(self, key, compute_fn):\n",
        "        now = time.time()\n",
        "        # Remove entradas expiradas\n",
        "        expired = [k for k, (_, ts) in self.cache.items() if now - ts > self.ttl]\n",
        "        for k in expired:\n",
        "            del self.cache[k]\n",
        "        # Retorna existente ou computa novo\n",
        "        if key in self.cache:\n",
        "            emb, ts = self.cache[key]\n",
        "            return emb\n",
        "        emb = compute_fn()\n",
        "        self.cache[key] = (emb, now)\n",
        "        return emb\n",
        "\n",
        "# Exemplo:\n",
        "# cache = EmbeddingCache(ttl_seconds=3600)\n",
        "# emb = cache.get('texto', lambda: model.embed('texto'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d0ca17e",
      "metadata": {
        "id": "1d0ca17e"
      },
      "outputs": [],
      "source": [
        "# 3. Widget de Latência Interativo\n",
        "import time\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "import plotly.express as px\n",
        "\n",
        "# Função de medição de latência (placeholder)\n",
        "def measure_latency(model_fn, prompt):\n",
        "    start = time.time()\n",
        "    _ = model_fn(prompt)\n",
        "    return (time.time() - start) * 1000  # ms\n",
        "\n",
        "# Widget interativo\n",
        "model_names = ['model_a', 'model_b']\n",
        "def get_model_fn(name):\n",
        "    # Retorna uma função que gera com o modelo; aqui simulamos\n",
        "    return lambda prompt: prompt[::-1]  # exemplo invertendo string\n",
        "\n",
        "dropdown = widgets.SelectMultiple(options=model_names, description='Modelos:')\n",
        "button = widgets.Button(description='Medir Latência')\n",
        "out = widgets.Output()\n",
        "\n",
        "def on_click(b):\n",
        "    with out:\n",
        "        clear_output()\n",
        "        latencies = {'model': [], 'latency_ms': []}\n",
        "        for name in dropdown.value:\n",
        "            fn = get_model_fn(name)\n",
        "            lat = measure_latency(fn, \"Teste de latência\")\n",
        "            latencies['model'].append(name)\n",
        "            latencies['latency_ms'].append(lat)\n",
        "        df = __import__('pandas').DataFrame(latencies)\n",
        "        fig = px.bar(df, x='model', y='latency_ms', title='Latência por Modelo')\n",
        "        fig.show()\n",
        "\n",
        "button.on_click(on_click)\n",
        "display(widgets.VBox([dropdown, button, out]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60cb2be3",
      "metadata": {
        "id": "60cb2be3"
      },
      "outputs": [],
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output, Markdown, Code\n",
        "\n",
        "# Define code challenges\n",
        "challenges = {\n",
        "    \"Quantização Custom\": {\n",
        "        \"desc\": (\"Implemente a quantização dinâmica convertendo pesos de um modelo para INT8 \"\n",
        "                 \"e realizando inferência, sem usar `torch.quantization`.\"),\n",
        "        \"starter\": (\"def dynamic_quantize_custom(model):\n",
        "\"\n",
        "                    \"    # Sua implementação aqui\n",
        "\"\n",
        "                    \"    return quantized_model\n",
        "\n",
        "\"\n",
        "                    \"# Exemplo de uso:\n",
        "\"\n",
        "                    \"quant_model = dynamic_quantize_custom(model)\n",
        "\"\n",
        "                    \"quant_model(input_ids)\")\n",
        "    },\n",
        "    \"Cache de Embeddings\": {\n",
        "        \"desc\": (\"Crie um cache de embeddings com dicionário que expira entradas após um tempo \"\n",
        "                 \"configurável (e.g., TTL em segundos).\"),\n",
        "        \"starter\": (\"class EmbeddingCache:\n",
        "\"\n",
        "                    \"    def __init__(self, ttl_seconds):\n",
        "\"\n",
        "                    \"        # Sua implementação aqui\n",
        "\"\n",
        "                    \"        pass\n",
        "\n",
        "\"\n",
        "                    \"    def get(self, key, compute_fn):\n",
        "\"\n",
        "                    \"        # Retorna embedding existente ou computa e armazena\n",
        "\"\n",
        "                    \"        pass\n",
        "\n",
        "\"\n",
        "                    \"# Uso:\n",
        "\"\n",
        "                    \"cache = EmbeddingCache(ttl_seconds=3600)\n",
        "\"\n",
        "                    \"emb = cache.get('texto', lambda: model.embed('texto'))\")\n",
        "    },\n",
        "    \"Widget de Latência\": {\n",
        "        \"desc\": (\"Desenvolva um widget interativo em Jupyter que recebe uma lista de modelos \"\n",
        "                 \"e mede suas latências em tempo real, exibindo um gráfico atualizado.\"),\n",
        "        \"starter\": (\"import time\n",
        "\"\n",
        "                    \"import ipywidgets as widgets\n",
        "\"\n",
        "                    \"from IPython.display import display, clear_output\n",
        "\n",
        "\"\n",
        "                    \"models = ['model1', 'model2']\n",
        "\"\n",
        "                    \"def measure_latency(model_name):\n",
        "\"\n",
        "                    \"    # Implementar medição\n",
        "\"\n",
        "                    \"    return latency_ms\n",
        "\n",
        "\"\n",
        "                    \"dropdown = widgets.SelectMultiple(options=models, description='Modelos')\n",
        "\"\n",
        "                    \"button = widgets.Button(description='Medir')\n",
        "\"\n",
        "                    \"output = widgets.Output()\n",
        "\n",
        "\"\n",
        "                    \"def on_click(b):\n",
        "\"\n",
        "                    \"    with output:\n",
        "\"\n",
        "                    \"        clear_output()\n",
        "\"\n",
        "                    \"        # Medir e plotar\n",
        "\"\n",
        "                    \"        pass\n",
        "\n",
        "\"\n",
        "                    \"button.on_click(on_click)\n",
        "\"\n",
        "                    \"display(dropdown, button, output)\")\n",
        "    }\n",
        "}\n",
        "\n",
        "# Widgets\n",
        "sel = widgets.Dropdown(options=list(challenges.keys()), description='Desafio:')\n",
        "btn = widgets.Button(description='Mostrar Desafio', button_style='info')\n",
        "out = widgets.Output()\n",
        "\n",
        "def show_challenge(b):\n",
        "    with out:\n",
        "        clear_output()\n",
        "        key = sel.value\n",
        "        info = challenges[key]\n",
        "        display(Markdown(f\"**Desafio:** {info['desc']}\"))\n",
        "        display(Code(info['starter'], language='python'))\n",
        "\n",
        "btn.on_click(show_challenge)\n",
        "display(widgets.VBox([sel, btn, out]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdd3af69",
      "metadata": {
        "id": "bdd3af69"
      },
      "outputs": [],
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output, Markdown, Code\n",
        "\n",
        "# Define hacks with description and code snippets\n",
        "hacks = {\n",
        "    \"GPU Offload\": {\n",
        "        \"desc\": \"Desloque pesos para CPU dinamicamente para economizar GPU memory.\",\n",
        "        \"code\": \"from transformers import AutoModel\\nmodel = AutoModel.from_pretrained(..., device_map='auto', offload_folder='offload')\"\n",
        "    },\n",
        "    \"Async Inference\": {\n",
        "        \"desc\": \"Use chamadas assíncronas para maximizar throughput em batch inferência.\",\n",
        "        \"code\": \"import asyncio\\nfrom transformers import pipeline\\npipe = pipeline('text-generation', model=...)\\nasync def gen(prompts):\\n    tasks = [asyncio.to_thread(pipe, p) for p in prompts]\\n    return await asyncio.gather(*tasks)\"\n",
        "    },\n",
        "    \"Shared Embedding Cache\": {\n",
        "        \"desc\": \"Reutilize embeddings comuns carregados em memória para múltiplas tarefas.\",\n",
        "        \"code\": \"emb_cache = {}\\ndef get_emb(text):\\n    if text not in emb_cache:\\n        emb_cache[text] = embed_model.encode(text)\\n    return emb_cache[text]\"\n",
        "    },\n",
        "    \"Advanced Prompt Templating\": {\n",
        "        \"desc\": \"Use loops e condicionais dentro do template para prompts dinâmicos.\",\n",
        "        \"code\": \"template = '''For input: {input}\\n{%- if len(ddx) > 1 %}\\nList DDx:\\n{% for d in ddx %}- {{d}}\\n{% endfor %}{% endif %}'''\\nfrom jinja2 import Template\\nprompt = Template(template).render(input='...', ddx=['A','B'])\"\n",
        "    },\n",
        "    \"Control Tokens\": {\n",
        "        \"desc\": \"Inclua tokens especiais como <|sep|>, <|startofplain|> para guiar a LLM.\",\n",
        "        \"code\": \"prompt = '<|startoftext|>Paciente apresenta...<|sep|>Pergunta: ...'\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# Widgets\n",
        "hack_select = widgets.Dropdown(options=list(hacks.keys()), description='Hack:')\n",
        "show_btn = widgets.Button(description='Mostrar Hack', button_style='info')\n",
        "out = widgets.Output()\n",
        "\n",
        "def show_hack(b):\n",
        "    with out:\n",
        "        clear_output()\n",
        "        key = hack_select.value\n",
        "        info = hacks[key]\n",
        "        display(Markdown(f\"**{key}**: {info['desc']}\"))\n",
        "        display(Code(info['code'], language='python'))\n",
        "\n",
        "show_btn.on_click(show_hack)\n",
        "display(widgets.VBox([hack_select, show_btn, out]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ecbc32a5",
      "metadata": {
        "id": "ecbc32a5"
      },
      "outputs": [],
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output, Markdown\n",
        "\n",
        "# Define aquecimento questions\n",
        "questions = [\n",
        "    {\n",
        "        \"q\": \"Qual método de quantização reduz dinamicamente os pesos para INT8?\",\n",
        "        \"options\": [\"static_int8\", \"dynamic_int8\", \"prune_magnitude\", \"fp16\"],\n",
        "        \"answer\": \"dynamic_int8\",\n",
        "        \"explanation\": \"Quantização dinâmica (dynamic_int8) converte tensores de peso em INT8 em tempo de execução sem calibração estática.\"\n",
        "    },\n",
        "    {\n",
        "        \"q\": \"Qual técnica de active learning seleciona amostras com maior incerteza?\",\n",
        "        \"options\": [\"margin_sampling\", \"uncertainty_sampling\", \"random_sampling\", \"entropy_sampling\"],\n",
        "        \"answer\": \"uncertainty_sampling\",\n",
        "        \"explanation\": \"Uncertainty sampling escolhe instâncias onde o modelo está menos confiante para rotulagem.\"\n",
        "    },\n",
        "    {\n",
        "        \"q\": \"Em RAG, qual etapa ocorre antes da geração pela LLM?\",\n",
        "        \"options\": [\"Indexação FAISS\", \"Pré-processamento\", \"Recuperação de documentos\", \"Tokenização\"],\n",
        "        \"answer\": \"Recuperação de documentos\",\n",
        "        \"explanation\": \"Após indexar embeddings, recupera-se os documentos relevantes (retrieval) antes de gerar a resposta.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "question_index = 0\n",
        "\n",
        "# Widgets\n",
        "question_label = widgets.HTML()\n",
        "options = widgets.RadioButtons(options=[], description='Opções:')\n",
        "submit_btn = widgets.Button(description='Submeter', button_style='info')\n",
        "feedback = widgets.Output()\n",
        "\n",
        "def load_question(idx):\n",
        "    question_label.value = f\"<b>Pergunta {idx+1}:</b> {questions[idx]['q']}\"\n",
        "    options.options = questions[idx]['options']\n",
        "    options.value = None\n",
        "    with feedback:\n",
        "        clear_output()\n",
        "\n",
        "def on_submit(b):\n",
        "    idx = question_index\n",
        "    selected = options.value\n",
        "    with feedback:\n",
        "        clear_output()\n",
        "        if selected == questions[idx]['answer']:\n",
        "            display(Markdown(f\"**Correto!** {questions[idx]['explanation']}\" ))\n",
        "        else:\n",
        "            display(Markdown(f\"**Incorreto.** {questions[idx]['explanation']}\" ))\n",
        "    # Move to next if exists\n",
        "    global question_index\n",
        "    if question_index < len(questions) - 1:\n",
        "        question_index += 1\n",
        "        load_question(question_index)\n",
        "    else:\n",
        "        with feedback:\n",
        "            display(Markdown(\"**Aquecimento concluído!** Agora você está pronto para exercícios avançados.\"))\n",
        "\n",
        "submit_btn.on_click(on_submit)\n",
        "\n",
        "# Initialize\n",
        "load_question(question_index)\n",
        "display(widgets.VBox([question_label, options, submit_btn, feedback]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13bb9a5e",
      "metadata": {
        "id": "13bb9a5e"
      },
      "outputs": [],
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output, Markdown\n",
        "\n",
        "# Concept definitions\n",
        "concepts = {\n",
        "    'Embeddings & Similaridade': (\n",
        "        \"Representações vetoriais de texto/imagem que permitem comparar semântica \"\n",
        "        \"via similaridade de cosseno.\",\n",
        "        \"Exemplo: usar `sentence-transformers` para comparar casos clínicos similares.\"\n",
        "    ),\n",
        "    'Fine-Tuning & Multi-Task Learning': (\n",
        "        \"Ajuste de LLM pré-treinados em dados específicos, podendo treinar para múltiplas tarefas.\",\n",
        "        \"Exemplo: treinar `T5` para QA e resumo simultaneamente.\"\n",
        "    ),\n",
        "    'RAG (Retrieval-Augmented Generation)': (\n",
        "        \"Combinação de recuperação de documentos relevantes com geração de texto pela LLM.\",\n",
        "        \"Exemplo: indexar guidelines médicas e usar GPT para respostas contextualizadas.\"\n",
        "    ),\n",
        "    'Quantização & Pruning': (\n",
        "        \"Redução do tamanho do modelo e acelerar inferência via quantização e remoção de pesos.\",\n",
        "        \"Exemplo: quantizar dinâmico INT8 e aplicar `prune` em camadas lineares.\"\n",
        "    ),\n",
        "    'Containerização & CI/CD': (\n",
        "        \"Encapsular serviço em container Docker e automatizar build/deploy com GitHub Actions.\",\n",
        "        \"Exemplo: criar Dockerfile e workflow para push em registry e deploy em Kubernetes.\"\n",
        "    ),\n",
        "    'Monitoramento & Alertas': (\n",
        "        \"Coleta de métricas (Prometheus), visualização (Grafana) e alertas (Slack).\",\n",
        "        \"Exemplo: alertar quando latência ultrapassa threshold definido.\"\n",
        "    ),\n",
        "    'Active Learning & Versionamento': (\n",
        "        \"Fluxo de anotação iterativa de dados, re-treinamento e controle de versões (DVC/MLflow).\",\n",
        "        \"Exemplo: amostrar instâncias incertas e versionar datasets após rotulagem.\"\n",
        "    ),\n",
        "    'Prompt Management & Model Testing': (\n",
        "        \"Criação e gerenciamento de templates de prompt, comparação de respostas entre LLMs.\",\n",
        "        \"Exemplo: usar TextAttack para test-over-testing e medir coerência entre modelos.\"\n",
        "    )\n",
        "}\n",
        "\n",
        "dropdown = widgets.Dropdown(options=list(concepts.keys()), description='Conceito:')\n",
        "output = widgets.Output()\n",
        "\n",
        "def show_concept(change):\n",
        "    with output:\n",
        "        clear_output()\n",
        "        name = change['new']\n",
        "        definition, example = concepts[name]\n",
        "        display(Markdown(f\"### {name}\n",
        "\n",
        "**Definição:** {definition}\n",
        "\n",
        "**Exemplo de aplicação:** {example}\"))\n",
        "\n",
        "dropdown.observe(show_concept, names='value')\n",
        "display(dropdown, output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb87aa51",
      "metadata": {
        "id": "eb87aa51"
      },
      "outputs": [],
      "source": [
        "# Instalação de dependências para comparação de modelos\n",
        "%pip install openai transformers sentence_transformers scikit-learn ipywidgets --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "501be9fe",
      "metadata": {
        "id": "501be9fe"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "import openai\n",
        "from transformers import pipeline\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# Load prompts\n",
        "prompt_upload = widgets.FileUpload(accept='.json', multiple=False, description='Upload Prompts JSON')\n",
        "model_select = widgets.SelectMultiple(\n",
        "    options=['gpt-3.5-turbo', 'gpt-4', 'google/flan-t5-base', 'facebook/bart-large-cnn'],\n",
        "    value=['gpt-3.5-turbo','google/flan-t5-base'],\n",
        "    description='Modelos:')\n",
        "run_btn = widgets.Button(description='▶️ Executar Testes', button_style='info')\n",
        "out = widgets.Output()\n",
        "\n",
        "# Semantic similarity model\n",
        "sim_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "def evaluate_models(b):\n",
        "    with out:\n",
        "        clear_output()\n",
        "        if not prompt_upload.value:\n",
        "            print(\"Faça upload de um arquivo JSON com prompts.\")\n",
        "            return\n",
        "        prompts = json.loads(next(iter(prompt_upload.value.values()))['content'])\n",
        "        # prompts expected list of strings\n",
        "        if not isinstance(prompts, list):\n",
        "            print(\"JSON deve ser uma lista de prompts.\")\n",
        "            return\n",
        "        results = {}\n",
        "        for model_name in model_select.value:\n",
        "            results[model_name] = []\n",
        "            print(f\"Chamando modelo {model_name}...\")\n",
        "            for prompt in prompts:\n",
        "                if model_name.startswith('gpt'):\n",
        "                    openai.api_key = os.getenv('OPENAI_API_KEY','')\n",
        "                    resp = openai.ChatCompletion.create(\n",
        "                        model=model_name,\n",
        "                        messages=[{'role':'user','content': prompt}],\n",
        "                        temperature=0.7,\n",
        "                        max_tokens=150\n",
        "                    )\n",
        "                    text = resp.choices[0].message.content\n",
        "                else:\n",
        "                    gen = pipeline('text2text-generation', model=model_name, max_length=150)\n",
        "                    text = gen(prompt)[0]['generated_text']\n",
        "                results[model_name].append(text)\n",
        "        # Display side by side\n",
        "        for idx, prompt in enumerate(prompts):\n",
        "            print(f\"=== Prompt {idx+1}: {prompt} ===\")\n",
        "            for model_name, outputs in results.items():\n",
        "                print(f\"[{model_name}]: {outputs[idx][:200]}...\")\n",
        "            # Similarity between first two models\n",
        "            if len(model_select.value) >= 2:\n",
        "                a = sim_model.encode(results[model_select.value[0]][idx], convert_to_tensor=True)\n",
        "                b = sim_model.encode(results[model_select.value[1]][idx], convert_to_tensor=True)\n",
        "                score = util.pytorch_cos_sim(a,b).item()\n",
        "                print(f\"Similaridade semântica entre {model_select.value[0]} e {model_select.value[1]}: {score:.3f}\")\n",
        "            print()\n",
        "\n",
        "run_btn.on_click(evaluate_models)\n",
        "display(widgets.VBox([prompt_upload, model_select, run_btn, out]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac54b414",
      "metadata": {
        "id": "ac54b414"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import ipywidgets as widgets\n",
        "import openai\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "# Load or init prompt templates\n",
        "templates = {}\n",
        "tpl_file = 'prompt_templates.json'\n",
        "if os.path.exists(tpl_file):\n",
        "    with open(tpl_file, 'r', encoding='utf-8') as f:\n",
        "        templates = json.load(f)\n",
        "\n",
        "# Widgets\n",
        "tpl_select = widgets.Dropdown(options=list(templates.keys()), description='Template:')\n",
        "new_tpl_name = widgets.Text(description='Novo Nome:')\n",
        "new_tpl_body = widgets.Textarea(description='Corpo (use {placeholder}):', layout=widgets.Layout(width='100%', height='80px'))\n",
        "temp_slider = widgets.FloatSlider(value=0.7, min=0, max=1, step=0.05, description='Temperatura:')\n",
        "max_tokens = widgets.IntSlider(value=150, min=10, max=1024, step=10, description='Max Tokens:')\n",
        "fill_area = widgets.Textarea(description='Valores JSON:', layout=widgets.Layout(width='100%', height='80px'),\n",
        "                              placeholder='{\\\"scenario\\\": \\\"...\\\"}')\n",
        "save_tpl_btn = widgets.Button(description='Salvar Template', button_style='success')\n",
        "run_btn = widgets.Button(description='▶️ Gerar com Prompt', button_style='info')\n",
        "out = widgets.Output()\n",
        "\n",
        "def save_template(b):\n",
        "    name = new_tpl_name.value.strip()\n",
        "    body = new_tpl_body.value.strip()\n",
        "    if not name or not body:\n",
        "        with out:\n",
        "            clear_output()\n",
        "            print(\"Nome e corpo do template obrigatórios.\")\n",
        "        return\n",
        "    templates[name] = body\n",
        "    with open(tpl_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(templates, f, ensure_ascii=False, indent=2)\n",
        "    tpl_select.options = list(templates.keys())\n",
        "    with out:\n",
        "        clear_output()\n",
        "        print(f\"Template '{name}' salvo.\")\n",
        "\n",
        "def run_prompt(b):\n",
        "    with out:\n",
        "        clear_output()\n",
        "        tpl = templates.get(tpl_select.value)\n",
        "        if not tpl:\n",
        "            print(\"Selecione um template válido.\")\n",
        "            return\n",
        "        try:\n",
        "            values = json.loads(fill_area.value)\n",
        "            prompt = tpl.format(**values)\n",
        "        except Exception as e:\n",
        "            print(\"Erro ao aplicar placeholders:\", e)\n",
        "            return\n",
        "        # Call LLM\n",
        "        openai.api_key = os.getenv('OPENAI_API_KEY','')\n",
        "        try:\n",
        "            resp = openai.ChatCompletion.create(\n",
        "                model='gpt-3.5-turbo',\n",
        "                messages=[{'role':'user','content': prompt}],\n",
        "                temperature=temp_slider.value,\n",
        "                max_tokens=max_tokens.value\n",
        "            )\n",
        "            print(\"=== Prompt ===\")\n",
        "            print(prompt)\n",
        "            print(\"\\n=== Resposta ===\")\n",
        "            print(resp.choices[0].message.content)\n",
        "        except Exception as e:\n",
        "            print(\"Erro na chamada da LLM:\", e)\n",
        "\n",
        "save_tpl_btn.on_click(save_template)\n",
        "run_btn.on_click(run_prompt)\n",
        "\n",
        "display(widgets.VBox([\n",
        "    widgets.HBox([tpl_select]),\n",
        "    widgets.Label(\"Criar Novo Template:\"),\n",
        "    new_tpl_name, new_tpl_body, save_tpl_btn,\n",
        "    widgets.Label(\"Valores de preenchimento (JSON):\"), fill_area,\n",
        "    widgets.HBox([temp_slider, max_tokens]),\n",
        "    run_btn, out\n",
        "]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b6c3a69",
      "metadata": {
        "id": "5b6c3a69"
      },
      "outputs": [],
      "source": [
        "# Instalação de dependências para data augmentation\n",
        "%pip install nlpaug fasttext transformers sentencepiece ipywidgets --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65f8bce0",
      "metadata": {
        "id": "65f8bce0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import nlpaug.augmenter.word as naw\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "# Load datasets\n",
        "def load_data(name):\n",
        "    if os.path.exists(f'{name}.json'):\n",
        "        with open(f'{name}.json','r',encoding='utf-8') as f:\n",
        "            return json.load(f)\n",
        "    return {}\n",
        "\n",
        "train_data = load_data('train')\n",
        "valid_data = load_data('valid')\n",
        "\n",
        "# Widgets\n",
        "dataset_choice = widgets.Dropdown(options=['train','valid'], description='Dataset:')\n",
        "methods = widgets.SelectMultiple(\n",
        "    options=[\n",
        "        'back_translation', 'synonym', 'contextual', 'random_deletion', 'random_insertion'\n",
        "    ],\n",
        "    value=['synonym'],\n",
        "    description='Métodos:')\n",
        "bt_src = widgets.Dropdown(options=['pt','en'], value='pt', description='BT Source:')\n",
        "bt_mid = widgets.Dropdown(options=['en','es'], value='en', description='BT Mid:')\n",
        "syn_model = widgets.Dropdown(options=['wordnet','contextual'], value='wordnet', description='Syn Model:')\n",
        "insert_prob = widgets.FloatSlider(value=0.1, min=0.01, max=0.5, step=0.01, description='Insert P:')\n",
        "delete_prob = widgets.FloatSlider(value=0.1, min=0.01, max=0.5, step=0.01, description='Delete P:')\n",
        "n_aug = widgets.IntSlider(value=2, min=1, max=5, step=1, description='Per Entrada:')\n",
        "augment_btn = widgets.Button(description='▶️ Augmentar', button_style='success')\n",
        "out = widgets.Output()\n",
        "\n",
        "# Initialize augmenters lazily\n",
        "def get_aug(method):\n",
        "    if method == 'back_translation':\n",
        "        return naw.BackTranslationAug(src_lang=bt_src.value, mid_lang=bt_mid.value)\n",
        "    if method == 'synonym':\n",
        "        return naw.SynonymAug(aug_src=syn_model.value)\n",
        "    if method == 'contextual':\n",
        "        return naw.ContextualWordEmbsAug(model_path='bert-base-uncased', action=\"substitute\")\n",
        "    if method == 'random_deletion':\n",
        "        return naw.RandomWordAug(action=\"delete\", aug_p=delete_prob.value)\n",
        "    if method == 'random_insertion':\n",
        "        return naw.RandomWordAug(action=\"insert\", aug_p=insert_prob.value)\n",
        "    return None\n",
        "\n",
        "def augment_data(b):\n",
        "    with out:\n",
        "        clear_output()\n",
        "        ds_name = dataset_choice.value\n",
        "        data = train_data if ds_name=='train' else valid_data\n",
        "        if not data:\n",
        "            print(f\"Dataset '{ds_name}' vazio ou não encontrado.\")\n",
        "            return\n",
        "        aug_data = {}\n",
        "        next_id = int(max(data.keys(), default='0')) + 1\n",
        "        # copy original\n",
        "        aug_data.update(data)\n",
        "        # apply augmentation\n",
        "        for cid, entry in data.items():\n",
        "            text = entry.get('scenario','')\n",
        "            for i in range(n_aug.value):\n",
        "                aug_text = text\n",
        "                for method in methods.value:\n",
        "                    aug = get_aug(method)\n",
        "                    if aug:\n",
        "                        aug_text = aug.augment(aug_text)\n",
        "                new_entry = entry.copy()\n",
        "                new_entry['scenario'] = aug_text\n",
        "                aug_data[str(next_id)] = new_entry\n",
        "                next_id += 1\n",
        "        # save augmented\n",
        "        out_file = f\"{ds_name}_aug.json\"\n",
        "        with open(out_file,'w',encoding='utf-8') as f:\n",
        "            json.dump(aug_data, f, ensure_ascii=False, indent=2)\n",
        "        print(f\"Aumento completo: {len(aug_data)-len(data)} exemplos gerados.\")\n",
        "        print(f\"Dataset aumentado salvo em '{out_file}'.\")\n",
        "        # preview\n",
        "        sample_keys = list(aug_data.keys())[-min(5, len(aug_data))]\n",
        "        print(\"Amostras geradas:\")\n",
        "        for k in sample_keys:\n",
        "            print(f\"{k}: {aug_data[k]['scenario']}\")\n",
        "\n",
        "augment_btn.on_click(augment_data)\n",
        "display(widgets.VBox([\n",
        "    dataset_choice, methods,\n",
        "    widgets.HBox([bt_src, bt_mid, syn_model]),\n",
        "    widgets.HBox([insert_prob, delete_prob]), n_aug,\n",
        "    augment_btn, out\n",
        "]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66b95826",
      "metadata": {
        "id": "66b95826"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "# Load or initialize JSON datasets\n",
        "def load_json(name):\n",
        "    if os.path.exists(f'{name}.json'):\n",
        "        with open(f'{name}.json','r',encoding='utf-8') as f:\n",
        "            return json.load(f)\n",
        "    return {}\n",
        "\n",
        "train_data = load_json('train')\n",
        "valid_data = load_json('valid')\n",
        "\n",
        "# Widgets for case entry\n",
        "dataset_select = widgets.RadioButtons(options=['train','valid'], description='Dataset:')\n",
        "scenario = widgets.Text(description='Cenário:', placeholder='Dor precordial há 47 min')\n",
        "ddx = widgets.Text(description='3 DDx:', placeholder='IAM, Dissecção aorta, DRGE')\n",
        "cd = widgets.Text(description='Condutas/Exames:', placeholder='ECG, marcadores, estratificação, SSVV, medicações')\n",
        "question = widgets.Text(description='Pergunta:', placeholder='Quanto tempo de dor?')\n",
        "answer = widgets.Text(description='Resposta:', placeholder='Agora já 48 min')\n",
        "add_btn = widgets.Button(description='Adicionar Caso', button_style='success')\n",
        "out = widgets.Output()\n",
        "\n",
        "def add_case(b):\n",
        "    with out:\n",
        "        clear_output()\n",
        "        ds = train_data if dataset_select.value=='train' else valid_data\n",
        "        # generate new ID\n",
        "        case_id = str(int(max(ds.keys(), default='0')) + 1)\n",
        "        # construct entry\n",
        "        entry = {\n",
        "            'scenario': scenario.value,\n",
        "            'ddx': [d.strip() for d in ddx.value.split(',') if d.strip()],\n",
        "            'cd': [c.strip() for c in cd.value.split(',') if c.strip()],\n",
        "            'follow_up': {question.value: answer.value}\n",
        "        }\n",
        "        ds[case_id] = entry\n",
        "        # save JSON\n",
        "        with open(f'{dataset_select.value}.json','w',encoding='utf-8') as f:\n",
        "            json.dump(ds, f, ensure_ascii=False, indent=2)\n",
        "        print(f\"Caso {case_id} adicionado ao '{dataset_select.value}.json'.\")\n",
        "        # show current dataset size\n",
        "        print(f\"Total train: {len(train_data)} casos; Total valid: {len(valid_data)} casos.\")\n",
        "\n",
        "add_btn.on_click(add_case)\n",
        "display(dataset_select, scenario, ddx, cd, question, answer, add_btn, out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db60e077",
      "metadata": {
        "id": "db60e077"
      },
      "outputs": [],
      "source": [
        "# Instalação de dependências para visualização de métricas\n",
        "%pip install mlflow pandas plotly ipywidgets --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3cfd9790",
      "metadata": {
        "id": "3cfd9790"
      },
      "outputs": [],
      "source": [
        "import mlflow\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "# Widgets to select experiment and runs\n",
        "client = mlflow.tracking.MlflowClient()\n",
        "experiments = client.list_experiments()\n",
        "exp_options = {exp.name: exp.experiment_id for exp in experiments}\n",
        "exp_select = widgets.Dropdown(options=exp_options, description='Experimento:')\n",
        "fetch_btn = widgets.Button(description='Buscar Runs', button_style='info')\n",
        "runs_select = widgets.SelectMultiple(description='Runs:', rows=5)\n",
        "plot_btn = widgets.Button(description='Plotar Métricas', button_style='success')\n",
        "out = widgets.Output()\n",
        "\n",
        "def fetch_runs(b):\n",
        "    with out:\n",
        "        clear_output()\n",
        "        exp_id = exp_select.value\n",
        "        runs = client.search_runs([exp_id], filter_string=\"\", max_results=50, order_by=[\"attributes.start_time DESC\"])\n",
        "        run_opts = {run.info.run_id: run.data.metrics for run in runs}\n",
        "        runs_select.options = run_opts\n",
        "        print(f\"Encontradas {len(runs)} runs para experimento '{exp_select.label}'.\")\n",
        "\n",
        "def plot_metrics(b):\n",
        "    with out:\n",
        "        clear_output()\n",
        "        selected = runs_select.value\n",
        "        if not selected:\n",
        "            print(\"Selecione ao menos uma run.\")\n",
        "            return\n",
        "        df_list = []\n",
        "        for run_id, metrics in runs_select.options:\n",
        "            if run_id in selected:\n",
        "                for metric, value in metrics.items():\n",
        "                    df_list.append({'run_id': run_id, 'metric': metric, 'value': value})\n",
        "        df = pd.DataFrame(df_list)\n",
        "        if df.empty:\n",
        "            print(\"Nenhum dado de métricas disponível.\")\n",
        "            return\n",
        "        fig = px.line(df, x='metric', y='value', color='run_id', markers=True, title='Comparação de Métricas por Run')\n",
        "        fig.show()\n",
        "\n",
        "fetch_btn.on_click(fetch_runs)\n",
        "plot_btn.on_click(plot_metrics)\n",
        "\n",
        "display(widgets.VBox([exp_select, fetch_btn, runs_select, plot_btn, out]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cffc0430",
      "metadata": {
        "id": "cffc0430"
      },
      "outputs": [],
      "source": [
        "# Instalação de dependências de AL avançado e versionamento\n",
        "%pip install modAL scikit-learn dvc mlflow ipywidgets pandas --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23427d2d",
      "metadata": {
        "id": "23427d2d"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import mlflow\n",
        "import dvc.api\n",
        "import ipywidgets as widgets\n",
        "from modAL.models import ActiveLearner\n",
        "from modAL.uncertainty import entropy_sampling\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "# Widgets\n",
        "sampling_strategy = widgets.Select(\n",
        "    options=['uncertainty', 'entropy', 'ensemble'],\n",
        "    value='entropy', description='Estratégia:')\n",
        "version_btn = widgets.Button(description='▶️ Salvar Versão', button_style='success')\n",
        "out = widgets.Output()\n",
        "\n",
        "# Initialize DVC/MLflow\n",
        "mlflow.set_experiment('bemrbriollmogy_AL')\n",
        "dvc_repo = dvc.api.Repo()\n",
        "\n",
        "def run_active_learning(b):\n",
        "    with out:\n",
        "        clear_output()\n",
        "        # Placeholder: Load unlabeled and train as earlier sections\n",
        "        # Simulate a batch\n",
        "        print(f\"Executando AL com estratégia {sampling_strategy.value} (simulado).\")\n",
        "\n",
        "def save_version(b):\n",
        "    with out:\n",
        "        clear_output()\n",
        "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "        version_name = f\"al_{timestamp}\"\n",
        "        # DVC add datasets\n",
        "        os.system('dvc add train.json valid.json unlabeled.json')\n",
        "        # Git commit (assuming git repo)\n",
        "        os.system(f'git add . && git commit -m \"AL version {version_name}\"')\n",
        "        # MLflow log\n",
        "        mlflow.start_run(run_name=version_name)\n",
        "        mlflow.log_param('sampling_strategy', sampling_strategy.value)\n",
        "        mlflow.log_artifact('train.json')\n",
        "        mlflow.log_artifact('valid.json')\n",
        "        mlflow.log_artifact('unlabeled.json')\n",
        "        mlflow.end_run()\n",
        "        print(f\"Versão '{version_name}' salva no DVC e MLflow.\")\n",
        "\n",
        "display(widgets.VBox([sampling_strategy, version_btn, out]))\n",
        "version_btn.on_click(save_version)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57e3ed92",
      "metadata": {
        "id": "57e3ed92"
      },
      "outputs": [],
      "source": [
        "# Instalação de dependências para relatórios\n",
        "%pip install plotly weasyprint pandas schedule ipywidgets requests --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0db7c37c",
      "metadata": {
        "id": "0db7c37c"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import schedule\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import requests\n",
        "from weasyprint import HTML\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output, Markdown\n",
        "\n",
        "# Widgets\n",
        "source = widgets.Dropdown(options=['Prometheus','W&B'], description='Fonte:')\n",
        "period = widgets.IntSlider(value=24, min=1, max=168, step=1, description='Período (h):')\n",
        "fmt = widgets.Select(options=['HTML','PDF','CSV'], description='Formato:')\n",
        "schedule_time = widgets.TimePicker(value=time.strftime(\"%H:00\"), description='Agendar:')\n",
        "email = widgets.Text(description='Enviar p/ email:')\n",
        "generate_btn = widgets.Button(description='Gerar Relatório', button_style='info')\n",
        "schedule_btn = widgets.Button(description='▶️ Agendar Relatório', button_style='success')\n",
        "out = widgets.Output()\n",
        "\n",
        "def fetch_prometheus(hours):\n",
        "    url = f\"http://localhost:9090/api/v1/query_range\"\n",
        "    end = int(time.time())\n",
        "    start = end - hours*3600\n",
        "    params = {'query':'request_latency_ms','start':start,'end':end,'step':str(int(hours*60))+'m'}\n",
        "    resp = requests.get(url, params=params).json()['data']['result'][0]['values']\n",
        "    df = pd.DataFrame(resp, columns=['timestamp','latency'])\n",
        "    df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s')\n",
        "    return df\n",
        "\n",
        "def fetch_wandb(hours):\n",
        "    # Placeholder: requires W&B API integration\n",
        "    # Simulate data\n",
        "    idx = pd.date_range(end=pd.Timestamp.now(), periods=hours, freq='H')\n",
        "    df = pd.DataFrame({'timestamp': idx, 'metric': [i*0.1 for i in range(len(idx))]})\n",
        "    return df\n",
        "\n",
        "def generate_report(_):\n",
        "    with out:\n",
        "        clear_output()\n",
        "        hrs = period.value\n",
        "        if source.value=='Prometheus':\n",
        "            df = fetch_prometheus(hrs)\n",
        "            title = 'Latency over last '+str(hrs)+'h'\n",
        "            fig = px.line(df, x='timestamp', y='latency', title=title)\n",
        "        else:\n",
        "            df = fetch_wandb(hrs)\n",
        "            title = 'W&B metric over last '+str(hrs)+'h'\n",
        "            fig = px.line(df, x='timestamp', y='metric', title=title)\n",
        "        # Export\n",
        "        if fmt.value=='CSV':\n",
        "            fname = 'report.csv'\n",
        "            df.to_csv(fname, index=False)\n",
        "            print(\"CSV saved:\", fname)\n",
        "        elif fmt.value=='HTML':\n",
        "            html = fig.to_html(full_html=True)\n",
        "            fname = 'report.html'\n",
        "            with open(fname,'w') as f: f.write(html)\n",
        "            print(\"HTML saved:\", fname)\n",
        "        else: # PDF\n",
        "            html = fig.to_html(full_html=True)\n",
        "            fname = 'report.pdf'\n",
        "            HTML(string=html).write_pdf(fname)\n",
        "            print(\"PDF saved:\", fname)\n",
        "        # Optionally send email (placeholder)\n",
        "        if email.value:\n",
        "            print(f\"Enviando relatório para {email.value} (simulado).\")\n",
        "\n",
        "generate_btn.on_click(generate_report)\n",
        "\n",
        "def schedule_report(_):\n",
        "    t = schedule_time.value.strftime(\"%H:%M\")\n",
        "    schedule.every().day.at(t).do(generate_report, None)\n",
        "    with out:\n",
        "        clear_output()\n",
        "        print(f\"Relatório agendado diariamente às {t}.\")\n",
        "\n",
        "schedule_btn.on_click(schedule_report)\n",
        "\n",
        "# Display widgets\n",
        "display(widgets.VBox([source, period, fmt, email,\n",
        "                      widgets.HBox([generate_btn, schedule_btn]), out]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41a3cfb8",
      "metadata": {
        "id": "41a3cfb8"
      },
      "outputs": [],
      "source": [
        "# Instalação de dependências de monitoramento\n",
        "%pip install prometheus_client slack_sdk requests schedule ipywidgets --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73fed033",
      "metadata": {
        "id": "73fed033"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import threading\n",
        "import requests\n",
        "import schedule\n",
        "import ipywidgets as widgets\n",
        "from prometheus_client import Summary, Counter, start_http_server\n",
        "from slack_sdk import WebClient\n",
        "from IPython.display import display, clear_output, Markdown\n",
        "\n",
        "# Widgets configuration\n",
        "prom_url = widgets.Text(value='http://localhost:9090', description='Prometheus URL:')\n",
        "grafana_url = widgets.Text(value='http://localhost:3000', description='Grafana URL:')\n",
        "dashboard_id = widgets.Text(value='1', description='Grafana Dashboard ID:')\n",
        "slack_webhook = widgets.Text(value=os.getenv('SLACK_WEBHOOK', ''), description='Slack Webhook:')\n",
        "latency_threshold = widgets.FloatText(value=500, description='Latency Threshold (ms):')\n",
        "error_threshold = widgets.IntText(value=5, description='Error Count Threshold:')\n",
        "start_btn = widgets.Button(description='▶️ Iniciar Monitoramento', button_style='success')\n",
        "out = widgets.Output()\n",
        "\n",
        "# Metrics placeholders\n",
        "REQUEST_LATENCY = Summary('request_latency_ms', 'Latency of requests in ms')\n",
        "ERROR_COUNTER = Counter('error_count', 'Number of errors')\n",
        "\n",
        "def fetch_metrics():\n",
        "    # Query Prometheus for recent metrics\n",
        "    try:\n",
        "        latency_q = f\"{prom_url.value}/api/v1/query?query=request_latency_ms\"\n",
        "        error_q = f\"{prom_url.value}/api/v1/query?query=error_count\"\n",
        "        lat_resp = requests.get(latency_q).json()\n",
        "        err_resp = requests.get(error_q).json()\n",
        "        latest_latency = float(lat_resp['data']['result'][0]['value'][1])\n",
        "        latest_errors = int(float(err_resp['data']['result'][0]['value'][1]))\n",
        "        return latest_latency, latest_errors\n",
        "    except:\n",
        "        return None, None\n",
        "\n",
        "def send_slack_alert(message):\n",
        "    client = WebClient(token=None)\n",
        "    requests.post(slack_webhook.value, json={'text': message})\n",
        "\n",
        "def monitor():\n",
        "    with out:\n",
        "        clear_output()\n",
        "        latency, errors = fetch_metrics()\n",
        "        display(Markdown(f\"**Current Latency:** {latency} ms\n",
        "**Error Count:** {errors}\"))\n",
        "        if latency and latency > latency_threshold.value:\n",
        "            send_slack_alert(f\":warning: Latency above threshold: {latency} ms\")\n",
        "        if errors and errors > error_threshold.value:\n",
        "            send_slack_alert(f\":red_circle: Error count high: {errors}\")\n",
        "\n",
        "def start_monitoring(b):\n",
        "    with out:\n",
        "        clear_output()\n",
        "        display(Markdown(\"**Monitoramento iniciado.**\"))\n",
        "    # Start Prometheus client to export in-process metrics\n",
        "    start_http_server(8000)\n",
        "    # Schedule fetch every minute\n",
        "    schedule.every(1).minutes.do(monitor)\n",
        "    def run_schedule():\n",
        "        while True:\n",
        "            schedule.run_pending()\n",
        "            time.sleep(1)\n",
        "    thread = threading.Thread(target=run_schedule, daemon=True)\n",
        "    thread.start()\n",
        "\n",
        "start_btn.on_click(start_monitoring)\n",
        "display(widgets.VBox([prom_url, grafana_url, dashboard_id, slack_webhook,\n",
        "                      latency_threshold, error_threshold,\n",
        "                      start_btn, out]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2f6578b",
      "metadata": {
        "id": "b2f6578b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import yaml\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "# Widgets for CI/CD & Kubernetes\n",
        "repo = widgets.Text(description='Repo (user/repo):', value='username/berm-repo')\n",
        "branch = widgets.Text(description='Branch:', value='main')\n",
        "registry = widgets.Text(description='Docker Registry:', value='docker.io/username')\n",
        "image = widgets.Text(description='Image Name:', value='bermbriollmogy:latest')\n",
        "chart_name = widgets.Text(description='Helm Chart:', value='berm-chart')\n",
        "namespace = widgets.Text(description='Namespace:', value='default')\n",
        "gen_btn = widgets.Button(description='Gerar CI e Helm', button_style='info')\n",
        "output = widgets.Output()\n",
        "\n",
        "def generate_ci_helm(b):\n",
        "    with output:\n",
        "        clear_output()\n",
        "        # GitHub Actions workflow\n",
        "        ci = {\n",
        "            'name': 'CI/CD Pipeline',\n",
        "            'on': {'push': {'branches': [branch.value]}},\n",
        "            'jobs': {\n",
        "                'build_push': {\n",
        "                    'runs-on': 'ubuntu-latest',\n",
        "                    'steps': [\n",
        "                        {'uses': 'actions/checkout@v2'},\n",
        "                        {'name': 'Set up QEMU', 'uses': 'docker/setup-qemu-action@v2'},\n",
        "                        {'name': 'Set up Docker Buildx', 'uses': 'docker/setup-buildx-action@v2'},\n",
        "                        {'name': 'Login to Registry', 'uses': 'docker/login-action@v2',\n",
        "                         'with': {'registry': registry.value, 'username': '${{secrets.DOCKER_USER}}', 'password': '${{secrets.DOCKER_PASS}}'}},\n",
        "                        {'name': 'Build and Push', 'uses': 'docker/build-push-action@v3',\n",
        "                         'with': {'context': '.', 'push': True, 'tags': f\"{registry.value}/{image.value}\"}}\n",
        "                    ]\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "        # Write workflow file\n",
        "        os.makedirs('.github/workflows', exist_ok=True)\n",
        "        with open('.github/workflows/ci-cd.yml','w') as f:\n",
        "            yaml.dump(ci, f)\n",
        "        print(\"✅ GitHub Actions workflow gerado em .github/workflows/ci-cd.yml\")\n",
        "        # Helm Chart skeleton\n",
        "        chart_dir = f'charts/{chart_name.value}'\n",
        "        os.makedirs(chart_dir, exist_ok=True)\n",
        "        chart_yaml = {\n",
        "            'apiVersion': 'v2',\n",
        "            'name': chart_name.value,\n",
        "            'version': '0.1.0',\n",
        "            'appVersion': image.value\n",
        "        }\n",
        "        values_yaml = {\n",
        "            'image': {'repository': f\"{registry.value}/{image.value.split(':')[0]}\", 'tag': image.value.split(':')[1]},\n",
        "            'service': {'type': 'ClusterIP', 'port': 80},\n",
        "            'ingress': {'enabled': False}\n",
        "        }\n",
        "        with open(f'{chart_dir}/Chart.yaml','w') as f:\n",
        "            yaml.dump(chart_yaml, f)\n",
        "        with open(f'{chart_dir}/values.yaml','w') as f:\n",
        "            yaml.dump(values_yaml, f)\n",
        "        print(f\"✅ Helm Chart skeleton criado em {chart_dir}/ (Chart.yaml e values.yaml)\")\n",
        "        print(f\"Use 'helm install {chart_name.value} {chart_dir} -n {namespace.value}' para deploy.\")\n",
        "\n",
        "gen_btn.on_click(generate_ci_helm)\n",
        "display(widgets.VBox([repo, branch, registry, image, chart_name, namespace, gen_btn, output]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77fae97f",
      "metadata": {
        "id": "77fae97f"
      },
      "outputs": [],
      "source": [
        "import ipywidgets as widgets\n",
        "import subprocess\n",
        "import os\n",
        "from IPython.display import display, clear_output, Markdown\n",
        "\n",
        "# Widgets de configuração\n",
        "base_image = widgets.Text(value='python:3.9-slim', description='Base Image:')\n",
        "model_dir = widgets.Text(value='mt_adv_model', description='Model Dir:')\n",
        "service_file = widgets.Text(value='app.py', description='Service File:')\n",
        "port = widgets.IntText(value=8080, description='Porta:')\n",
        "image_tag = widgets.Text(value='berm_v2:latest', description='Image Tag:')\n",
        "gen_btn = widgets.Button(description='Gerar Dockerfile', button_style='primary')\n",
        "build_btn = widgets.Button(description='Build Image', button_style='info')\n",
        "run_btn = widgets.Button(description='Run Container', button_style='success')\n",
        "test_btn = widgets.Button(description='Test Endpoints', button_style='warning')\n",
        "out = widgets.Output()\n",
        "\n",
        "def generate_dockerfile(b):\n",
        "    with out:\n",
        "        clear_output()\n",
        "        df = f\"\"\"FROM {base_image.value}\n",
        "WORKDIR /app\n",
        "COPY {model_dir.value} /app/model\n",
        "COPY {service_file.value} /app/{service_file.value}\n",
        "RUN pip install fastapi uvicorn transformers torch prometheus-client\n",
        "EXPOSE {port.value}\n",
        "CMD [\"uvicorn\", \"{service_file.replace('.py','')}:app\", \"--host\", \"0.0.0.0\", \"--port\", \"{port.value}\"]\"\"\"\n",
        "        with open('Dockerfile','w') as f:\n",
        "            f.write(df)\n",
        "        print(\"✅ Dockerfile gerado:\")\n",
        "        print(Markdown(f\"```\n",
        "{df}\n",
        "```\"))\n",
        "\n",
        "def build_image(b):\n",
        "    with out:\n",
        "        clear_output()\n",
        "        tag = image_tag.value\n",
        "        print(f\"Building image {tag}...\")\n",
        "        subprocess.run(['docker','build','-t',tag,'.'], check=False)\n",
        "        print(\"✅ Imagem construída.\")\n",
        "\n",
        "def run_container(b):\n",
        "    with out:\n",
        "        clear_output()\n",
        "        tag = image_tag.value\n",
        "        print(f\"Running container {tag} on port {port.value}...\")\n",
        "        subprocess.run(['docker','run','-d','-p',f\"{port.value}:{port.value}\",tag], check=False)\n",
        "        print(\"✅ Container em execução.\")\n",
        "\n",
        "def test_endpoints(b):\n",
        "    with out:\n",
        "        clear_output()\n",
        "        import requests\n",
        "        base = f\"http://localhost:{port.value}\"\n",
        "        endpoints = ['/health','/metrics','/predict']\n",
        "        for ep in endpoints:\n",
        "            try:\n",
        "                if ep == '/predict':\n",
        "                    resp = requests.post(base+ep, json={'input':'test'})\n",
        "                else:\n",
        "                    resp = requests.get(base+ep)\n",
        "                print(f\"{ep} -> status {resp.status_code}\")\n",
        "            except Exception as e:\n",
        "                print(f\"{ep} -> erro: {e}\")\n",
        "\n",
        "gen_btn.on_click(generate_dockerfile)\n",
        "build_btn.on_click(build_image)\n",
        "run_btn.on_click(run_container)\n",
        "test_btn.on_click(test_endpoints)\n",
        "\n",
        "display(widgets.VBox([\n",
        "    base_image, model_dir, service_file, port, image_tag,\n",
        "    widgets.HBox([gen_btn, build_btn, run_btn, test_btn]),\n",
        "    out\n",
        "]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8110762f",
      "metadata": {
        "id": "8110762f"
      },
      "outputs": [],
      "source": [
        "# Instalação de dependências para quantização e pruning\n",
        "%pip install torch transformers ipywidgets --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83b33fe1",
      "metadata": {
        "id": "83b33fe1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import torch\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "from torch.nn.utils import prune\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "import pandas as pd\n",
        "\n",
        "# Widgets\n",
        "model_dir = widgets.Text(value='ft_basic_model', description='Model Dir:')\n",
        "methods = widgets.SelectMultiple(\n",
        "    options=['dynamic_int8', 'static_int8', 'prune_magnitude'],\n",
        "    value=['dynamic_int8', 'prune_magnitude'],\n",
        "    description='Methods:')\n",
        "prune_amount = widgets.FloatSlider(value=0.2, min=0.0, max=0.9, step=0.1, description='Prune Amount:')\n",
        "run_btn = widgets.Button(description='▶️ Apply', button_style='success')\n",
        "out = widgets.Output()\n",
        "\n",
        "def measure_latency(model, tokenizer):\n",
        "    sample = \"cenario de teste\"\n",
        "    inputs = tokenizer(sample, return_tensors='pt')\n",
        "    start = time.time()\n",
        "    _ = model.generate(**inputs)\n",
        "    return (time.time() - start) * 1000\n",
        "\n",
        "def on_apply(b):\n",
        "    with out:\n",
        "        clear_output()\n",
        "        base = model_dir.value\n",
        "        tokenizer = AutoTokenizer.from_pretrained(base)\n",
        "        model = AutoModelForSeq2SeqLM.from_pretrained(base)\n",
        "        report = []\n",
        "        # Original\n",
        "        orig_size = os.path.getsize(os.path.join(base, 'pytorch_model.bin')) / 1e6\n",
        "        orig_lat = measure_latency(model, tokenizer)\n",
        "        report.append({'method': 'original', 'size_mb': orig_size, 'latency_ms': orig_lat})\n",
        "        # Apply methods\n",
        "        for m in methods.value:\n",
        "            if m == 'dynamic_int8':\n",
        "                qm = torch.quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)\n",
        "                name = f\"{base}_dyn_int8\"\n",
        "                qm.save_pretrained(name)\n",
        "                size = os.path.getsize(os.path.join(name, 'pytorch_model.bin')) / 1e6\n",
        "                lat = measure_latency(qm, tokenizer)\n",
        "                report.append({'method': 'dynamic_int8', 'size_mb': size, 'latency_ms': lat})\n",
        "            elif m == 'static_int8':\n",
        "                # Static quantization requires model preparation; using dynamic as placeholder\n",
        "                qs = torch.quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)\n",
        "                name = f\"{base}_stat_int8\"\n",
        "                qs.save_pretrained(name)\n",
        "                size = os.path.getsize(os.path.join(name, 'pytorch_model.bin')) / 1e6\n",
        "                lat = measure_latency(qs, tokenizer)\n",
        "                report.append({'method': 'static_int8', 'size_mb': size, 'latency_ms': lat})\n",
        "            elif m == 'prune_magnitude':\n",
        "                # Magnitude pruning on linear layers\n",
        "                pruned_model = AutoModelForSeq2SeqLM.from_pretrained(base)\n",
        "                parameters_to_prune = []\n",
        "                for module in pruned_model.modules():\n",
        "                    if isinstance(module, torch.nn.Linear):\n",
        "                        parameters_to_prune.append((module, 'weight'))\n",
        "                prune.global_unstructured(\n",
        "                    parameters_to_prune,\n",
        "                    pruning_method=prune.L1Unstructured,\n",
        "                    amount=prune_amount.value\n",
        "                )\n",
        "                name = f\"{base}_pruned_{int(prune_amount.value*100)}\"\n",
        "                pruned_model.save_pretrained(name)\n",
        "                size = os.path.getsize(os.path.join(name, 'pytorch_model.bin')) / 1e6\n",
        "                lat = measure_latency(pruned_model, tokenizer)\n",
        "                report.append({'method': f'prune_{int(prune_amount.value*100)}%', 'size_mb': size, 'latency_ms': lat})\n",
        "        df = pd.DataFrame(report)\n",
        "        display(df)\n",
        "        df.to_csv('quant_prune_report.csv', index=False)\n",
        "        print(\"✅ Report saved as quant_prune_report.csv\")\n",
        "\n",
        "run_btn.on_click(on_apply)\n",
        "display(widgets.VBox([model_dir, methods, prune_amount, run_btn, out]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa79aa87",
      "metadata": {
        "id": "fa79aa87"
      },
      "outputs": [],
      "source": [
        "# Instalação de dependências\n",
        "%pip install optuna wandb transformers datasets accelerate ipywidgets --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98ee74a4",
      "metadata": {
        "id": "98ee74a4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import optuna\n",
        "import wandb\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, TrainingArguments, Trainer\n",
        "from datasets import load_from_disk\n",
        "\n",
        "# Widgets for configuration\n",
        "model_dir = widgets.Dropdown(\n",
        "    options=['ft_basic_model', 'mt_adv_model'],\n",
        "    value='ft_basic_model',\n",
        "    description='Model Dir:')\n",
        "train_path = widgets.Text(value='ft_data/train', description='Train Path:')\n",
        "valid_path = widgets.Text(value='ft_data/valid', description='Valid Path:')\n",
        "n_trials = widgets.IntText(value=10, description='Trials:')\n",
        "lr_range = widgets.FloatRangeSlider(\n",
        "    value=[1e-5, 5e-5], min=1e-6, max=1e-3, step=1e-6, description='LR Range:')\n",
        "batch_sizes = widgets.Text(value='4,8,16', description='Batch Sizes:')\n",
        "wd_range = widgets.FloatRangeSlider(\n",
        "    value=[0.0, 0.01], min=0.0, max=0.1, step=0.001, description='WD Range:')\n",
        "sampler = widgets.Dropdown(options=['TPE','Random'], value='TPE', description='Sampler:')\n",
        "use_wandb = widgets.Checkbox(value=True, description='Use W&B')\n",
        "run_btn = widgets.Button(description='▶️ Run Optimization', button_style='info')\n",
        "out = widgets.Output()\n",
        "\n",
        "def objective(trial):\n",
        "    # Start W&B run\n",
        "    if use_wandb.value:\n",
        "        wandb.init(project='berm-opt', reinit=True)\n",
        "    # Suggest params\n",
        "    lr = trial.suggest_loguniform('learning_rate', lr_range.value[0], lr_range.value[1])\n",
        "    bs = int(trial.suggest_categorical('batch_size', [int(x) for x in batch_sizes.value.split(',')]))\n",
        "    wd = trial.suggest_loguniform('weight_decay', wd_range.value[0], wd_range.value[1])\n",
        "    epochs = trial.suggest_int('epochs', 1, 3)\n",
        "    # Load datasets\n",
        "    train_ds = load_from_disk(train_path.value)\n",
        "    valid_ds = load_from_disk(valid_path.value)\n",
        "    # Load model and tokenizer\n",
        "    tok = AutoTokenizer.from_pretrained(model_dir.value)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(model_dir.value)\n",
        "    # Define training args\n",
        "    args = TrainingArguments(\n",
        "        output_dir=f\"opt_{trial.number}\",\n",
        "        num_train_epochs=epochs,\n",
        "        per_device_train_batch_size=bs,\n",
        "        per_device_eval_batch_size=bs,\n",
        "        learning_rate=lr,\n",
        "        weight_decay=wd,\n",
        "        evaluation_strategy='steps',\n",
        "        eval_steps=50,\n",
        "        logging_steps=20,\n",
        "        save_strategy='no',\n",
        "        load_best_model_at_end=True,\n",
        "        report_to=['wandb'] if use_wandb.value else []\n",
        "    )\n",
        "    # Initialize Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        train_dataset=train_ds,\n",
        "        eval_dataset=valid_ds,\n",
        "        tokenizer=tok\n",
        "    )\n",
        "    # Train\n",
        "    trainer.train()\n",
        "    # Evaluate\n",
        "    metrics = trainer.evaluate()\n",
        "    loss = metrics.get('eval_loss')\n",
        "    # Log to W&B\n",
        "    if use_wandb.value:\n",
        "        wandb.log({'eval_loss': loss, 'learning_rate': lr, 'batch_size': bs, 'weight_decay': wd})\n",
        "        wandb.finish()\n",
        "    # Report\n",
        "    return loss\n",
        "\n",
        "def run_optimization(b):\n",
        "    with out:\n",
        "        clear_output()\n",
        "        # Configure sampler\n",
        "        if sampler.value == 'TPE':\n",
        "            sampler_obj = optuna.samplers.TPESampler()\n",
        "        else:\n",
        "            sampler_obj = optuna.samplers.RandomSampler()\n",
        "        study = optuna.create_study(\n",
        "            direction='minimize',\n",
        "            sampler=sampler_obj\n",
        "        )\n",
        "        study.optimize(objective, n_trials=n_trials.value)\n",
        "        print(\"Best params:\", study.best_params)\n",
        "        df = study.trials_dataframe(attrs=('number','value','params'))\n",
        "        display(df.head())\n",
        "        df.to_csv('optuna_study.csv', index=False)\n",
        "        print(\"Results saved to optuna_study.csv\")\n",
        "\n",
        "run_btn.on_click(run_optimization)\n",
        "display(widgets.VBox([\n",
        "    model_dir, train_path, valid_path,\n",
        "    n_trials, lr_range, batch_sizes, wd_range,\n",
        "    sampler, use_wandb, run_btn, out\n",
        "]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0690f50a",
      "metadata": {
        "id": "0690f50a"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import io\n",
        "import torch\n",
        "import faiss\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from transformers import AutoTokenizer, AutoModel, CLIPProcessor, CLIPModel\n",
        "import openai\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output, Markdown\n",
        "\n",
        "# Widgets for detailed chain-of-thought\n",
        "question_input = widgets.Textarea(\n",
        "    description='Pergunta:',\n",
        "    layout=widgets.Layout(width='100%', height='60px'),\n",
        "    placeholder='Descreva a pergunta clínica aqui...'\n",
        ")\n",
        "style_select = widgets.Dropdown(\n",
        "    options=['Breve', 'Detalhado'],\n",
        "    value='Detalhado',\n",
        "    description='Estilo:'\n",
        ")\n",
        "run_btn = widgets.Button(description='▶️ Gerar Raciocínio', button_style='info')\n",
        "out = widgets.Output()\n",
        "\n",
        "# Assume index, texts, images from Section 7a are available globally\n",
        "\n",
        "def generate_chain_of_thought(b):\n",
        "    with out:\n",
        "        clear_output()\n",
        "        q = question_input.value.strip()\n",
        "        if not q:\n",
        "            print(\"Insira uma pergunta para gerar o raciocínio.\")\n",
        "            return\n",
        "        # Retrieve context as in Section 7a\n",
        "        # For simplicity, reuse last index build if exists, else error\n",
        "        try:\n",
        "            idx, text_embs, img_embs  # use variables from 7a\n",
        "            texts  # list of texts\n",
        "            images  # list of PIL images\n",
        "        except NameError:\n",
        "            print(\"Execute a Seção 7a primeiro para construir o índice multimodal.\")\n",
        "            return\n",
        "        # Embed question with text embedder\n",
        "        tokenizer = AutoTokenizer.from_pretrained(embed_text_model.value)\n",
        "        model = AutoModel.from_pretrained(embed_text_model.value)\n",
        "        inputs = tokenizer(q, return_tensors='pt', truncation=True, padding=True)\n",
        "        with torch.no_grad():\n",
        "            q_emb = model(**inputs).last_hidden_state.mean(dim=1).cpu().numpy()\n",
        "        faiss.normalize_L2(q_emb)\n",
        "        D, I = idx.search(q_emb, top_k.value)\n",
        "        # Prepare context snippet markdown\n",
        "        md = \"### Contexto Recuperado:\n",
        "\"\n",
        "        for i in I[0]:\n",
        "            if i < len(texts):\n",
        "                md += f\"- Texto: {texts[i][:200]}...\n",
        "\"\n",
        "            else:\n",
        "                img = images[i - len(texts)]\n",
        "                # Save image temporarily\n",
        "                img_path = f\"retrieved_{i}.png\"\n",
        "                img.save(img_path)\n",
        "                md += f\"- Imagem: ![]({img_path})\n",
        "\"\n",
        "        md += \"\n",
        "\"\n",
        "        display(Markdown(md))\n",
        "        # Prepare OpenAI prompt with chain-of-thought instruction\n",
        "        openai.api_key = os.getenv('OPENAI_API_KEY','')\n",
        "        instruction = \"Forneça um raciocínio passo-a-passo\" + (\" e detalhado.\" if style_select.value=='Detalhado' else \".\")\n",
        "        full_prompt = f\"{instruction}\\nContexto:\\n\"\n",
        "        for i in I[0]:\n",
        "            if i < len(texts):\n",
        "                full_prompt += texts[i] + \"\\n\"\n",
        "        full_prompt += f\"Pergunta: {q}\"\n",
        "        # Call API\n",
        "        try:\n",
        "            resp = openai.ChatCompletion.create(\n",
        "                model=llm_model.value,\n",
        "                messages=[\n",
        "                    {'role':'system','content':'Você é um assistente médico que explica seu raciocínio.'},\n",
        "                    {'role':'user','content':full_prompt}\n",
        "                ],\n",
        "                temperature=0.7,\n",
        "                max_tokens=512\n",
        "            )\n",
        "            answer = resp.choices[0].message.content\n",
        "            print(\"### Raciocínio Gerado:\n",
        "\")\n",
        "            print(answer)\n",
        "        except Exception as e:\n",
        "            print(\"Erro na chamada OpenAI:\", e)\n",
        "\n",
        "run_btn.on_click(generate_chain_of_thought)\n",
        "display(widgets.VBox([question_input, style_select, run_btn, out]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d83c6faf",
      "metadata": {
        "id": "d83c6faf"
      },
      "outputs": [],
      "source": [
        "# Instalação de dependências para RAG multimodal\n",
        "%pip install faiss-cpu transformers openai pillow torchvision timm ipywidgets --quiet\n",
        "\n",
        "import io\n",
        "import os\n",
        "import torch\n",
        "import faiss\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from transformers import AutoTokenizer, AutoModel, CLIPProcessor, CLIPModel, pipeline\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "# Widgets for uploading modalities\n",
        "text_upload = widgets.FileUpload(accept='.txt,.csv', multiple=False, description='Texto clínico')\n",
        "image_upload = widgets.FileUpload(accept='.png,.jpg,.jpeg,.dcm', multiple=True, description='Imagens')\n",
        "embed_text_model = widgets.Text(value='sentence-transformers/all-MiniLM-L6-v2', description='Text Embedder:')\n",
        "embed_image_model = widgets.Text(value='openai/clip-vit-base-patch32', description='Image Embedder:')\n",
        "llm_model = widgets.Text(value='gpt-3.5-turbo', description='LLM para Geração:')\n",
        "top_k = widgets.IntSlider(value=3, min=1, max=10, step=1, description='Top-k:')\n",
        "run_btn = widgets.Button(description='▶️ Construir RAG', button_style='success')\n",
        "out = widgets.Output()\n",
        "\n",
        "def build_multimodal_index(texts, images, text_model, image_model):\n",
        "    # Text embeddings\n",
        "    tokenizer = AutoTokenizer.from_pretrained(text_model)\n",
        "    model = AutoModel.from_pretrained(text_model)\n",
        "    text_inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True)\n",
        "    with torch.no_grad():\n",
        "        text_embs = model(**text_inputs).last_hidden_state.mean(dim=1).cpu().numpy()\n",
        "    # Image embeddings\n",
        "    clip = CLIPModel.from_pretrained(image_model)\n",
        "    proc = CLIPProcessor.from_pretrained(image_model)\n",
        "    img_embs = []\n",
        "    for img in images:\n",
        "        if img.mode != 'RGB':\n",
        "            img = img.convert('RGB')\n",
        "        inputs = proc(images=img, return_tensors='pt')\n",
        "        with torch.no_grad():\n",
        "            emb = clip.get_image_features(**inputs).cpu().numpy()\n",
        "        img_embs.append(emb[0])\n",
        "    img_embs = np.vstack(img_embs) if img_embs else np.zeros((0, text_embs.shape[1]))\n",
        "    # Combine and index\n",
        "    all_embs = np.vstack([text_embs, img_embs])\n",
        "    index = faiss.IndexFlatIP(all_embs.shape[1])\n",
        "    faiss.normalize_L2(all_embs)\n",
        "    index.add(all_embs)\n",
        "    return index, text_embs, img_embs\n",
        "\n",
        "def on_run(b):\n",
        "    with out:\n",
        "        clear_output()\n",
        "        # Load texts\n",
        "        texts = []\n",
        "        if text_upload.value:\n",
        "            name, info = next(iter(text_upload.value.items()))\n",
        "            data = info['content'].decode('utf-8').splitlines()\n",
        "            texts = [line for line in data if line.strip()]\n",
        "        # Load images\n",
        "        images = []\n",
        "        for fname, info in image_upload.value.items():\n",
        "            try:\n",
        "                img = Image.open(io.BytesIO(info['content']))\n",
        "                images.append(img)\n",
        "            except:\n",
        "                pass\n",
        "        if not texts and not images:\n",
        "            print(\"⚠️ Faça upload de texto ou imagens.\")\n",
        "            return\n",
        "        # Build index\n",
        "        idx, tel, iel = build_multimodal_index(\n",
        "            texts, images, embed_text_model.value, embed_image_model.value)\n",
        "        print(\"Index multimodal construído com\", idx.ntotal, \"embeddings.\")\n",
        "        # Query first text as example\n",
        "        query_emb = tel[0].reshape(1,-1) if texts else iel[0].reshape(1,-1)\n",
        "        faiss.normalize_L2(query_emb)\n",
        "        D, I = idx.search(query_emb, top_k.value)\n",
        "        # Retrieve modalities\n",
        "        retrieved = []\n",
        "        for i in I[0]:\n",
        "            if i < len(texts):\n",
        "                retrieved.append((\"Texto\", texts[i]))\n",
        "            else:\n",
        "                img_idx = i - len(texts)\n",
        "                retrieved.append((\"Imagem\", images[img_idx]))\n",
        "        # Display retrieved\n",
        "        print(\"### Top-k Recuperados ###\")\n",
        "        for modality, item in retrieved:\n",
        "            if modality == \"Texto\":\n",
        "                print(f\"Texto: {item}\")\n",
        "            else:\n",
        "                display(item)\n",
        "        # Generate answer via LLM with chain-of-thought\n",
        "        import openai\n",
        "        openai.api_key = os.getenv('OPENAI_API_KEY','')\n",
        "        prompt = \"Você é um assistente médico. Baseado nos itens recuperados, responda com raciocínio passo-a-passo:\\n\"\n",
        "        for modality, item in retrieved:\n",
        "            if modality==\"Texto\":\n",
        "                prompt += f\"Texto: {item}\\n\"\n",
        "            else:\n",
        "                prompt += f\"[Imagem exibida]\\n\"\n",
        "        prompt += \"Question: resumir achados e diagnóstico provável.\"\n",
        "        resp = openai.ChatCompletion.create(\n",
        "            model=llm_model.value,\n",
        "            messages=[{\"role\":\"user\",\"content\":prompt}]\n",
        "        )\n",
        "        print(\"\\n### Resposta com Chain-of-Thought ###\")\n",
        "        print(resp.choices[0].message.content)\n",
        "\n",
        "run_btn.on_click(on_run)\n",
        "display(widgets.VBox([\n",
        "    text_upload, image_upload,\n",
        "    embed_text_model, embed_image_model,\n",
        "    llm_model, top_k, run_btn, out\n",
        "]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e65f734",
      "metadata": {
        "id": "7e65f734"
      },
      "outputs": [],
      "source": [
        "# Instalação de dependências multi-task avançado\n",
        "%pip install transformers datasets accelerate torch ipywidgets --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15a8f285",
      "metadata": {
        "id": "15a8f285"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import json\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForSeq2SeqLM,\n",
        "    TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
        ")\n",
        "\n",
        "# Widgets\n",
        "mt_upload = widgets.FileUpload(\n",
        "    accept='.json', multiple=False, description='Upload JSON multi-task')\n",
        "tasks = widgets.SelectMultiple(\n",
        "    options=['qa', 'summary', 'classification'],\n",
        "    value=['qa', 'summary', 'classification'],\n",
        "    description='Tarefas:')\n",
        "model_select = widgets.Dropdown(\n",
        "    options=['google/flan-t5-base', 't5-small', 'facebook/bart-base'],\n",
        "    value='google/flan-t5-base', description='Modelo:')\n",
        "epochs = widgets.IntText(value=3, description='Épocas:')\n",
        "batch_size = widgets.IntText(value=4, description='Batch Size:')\n",
        "lr = widgets.FloatText(value=5e-5, description='Learning Rate:')\n",
        "start_btn = widgets.Button(description='▶️ Treinar Multi-Task', button_style='success')\n",
        "output = widgets.Output()\n",
        "\n",
        "def run_multitask(b):\n",
        "    with output:\n",
        "        clear_output()\n",
        "        if not mt_upload.value:\n",
        "            print(\"⚠️ Faça upload do JSON multi-task.\")\n",
        "            return\n",
        "        # Load multi-task data\n",
        "        data = json.load(io.BytesIO(mt_upload.value[next(iter(mt_upload.value))]['content']))\n",
        "        # Build list of examples for each task\n",
        "        examples = []\n",
        "        for entry in data:\n",
        "            inp = entry.get('input', '')\n",
        "            for task in tasks.value:\n",
        "                trg = entry.get(task, '')\n",
        "                if trg:\n",
        "                    prompt = f\"{task}: {inp}\"\n",
        "                    examples.append({'prompt': prompt, 'target': trg})\n",
        "        if not examples:\n",
        "            print(\"Nenhum exemplo válido para as tarefas selecionadas.\")\n",
        "            return\n",
        "        # Create dataset\n",
        "        df = Dataset.from_list(examples)\n",
        "        # Tokenizer and model\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_select.value)\n",
        "        model = AutoModelForSeq2SeqLM.from_pretrained(model_select.value)\n",
        "        # Preprocess function\n",
        "        def preprocess(batch):\n",
        "            inputs = tokenizer(\n",
        "                batch['prompt'],\n",
        "                padding='max_length', truncation=True, max_length=256)\n",
        "            targets = tokenizer(\n",
        "                batch['target'],\n",
        "                padding='max_length', truncation=True, max_length=64)\n",
        "            batch = {\n",
        "                'input_ids': inputs['input_ids'],\n",
        "                'attention_mask': inputs['attention_mask'],\n",
        "                'labels': targets['input_ids'],\n",
        "            }\n",
        "            return batch\n",
        "        ds = df.map(preprocess, batched=True, remove_columns=['prompt','target'])\n",
        "        # Data collator\n",
        "        data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
        "        # Training arguments\n",
        "        args = TrainingArguments(\n",
        "            output_dir='mt_adv',\n",
        "            num_train_epochs=epochs.value,\n",
        "            per_device_train_batch_size=batch_size.value,\n",
        "            learning_rate=lr.value,\n",
        "            logging_steps=10,\n",
        "            evaluation_strategy='no',\n",
        "            save_total_limit=2,\n",
        "            report_to=[]\n",
        "        )\n",
        "        trainer = Trainer(\n",
        "            model=model,\n",
        "            args=args,\n",
        "            train_dataset=ds,\n",
        "            data_collator=data_collator,\n",
        "            tokenizer=tokenizer\n",
        "        )\n",
        "        print(\"🔄 Treinamento Multi-Task iniciado...\")\n",
        "        trainer.train()\n",
        "        trainer.save_model('mt_adv_model')\n",
        "        print(\"✅ Treinamento concluído. Modelo salvo em 'mt_adv_model'.\")\n",
        "\n",
        "start_btn.on_click(run_multitask)\n",
        "display(widgets.VBox([\n",
        "    mt_upload, tasks, model_select,\n",
        "    epochs, batch_size, lr,\n",
        "    start_btn, output\n",
        "]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44a60571",
      "metadata": {
        "id": "44a60571"
      },
      "outputs": [],
      "source": [
        "# Instalação de dependências para fine-tuning\n",
        "%pip install transformers datasets accelerate ipywidgets --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "915cd65e",
      "metadata": {
        "id": "915cd65e"
      },
      "outputs": [],
      "source": [
        "import io, json\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, TrainingArguments, Trainer\n",
        "\n",
        "# Widgets\n",
        "train_upload = widgets.FileUpload(accept='.json', multiple=False, description='train.json')\n",
        "valid_upload = widgets.FileUpload(accept='.json', multiple=False, description='valid.json')\n",
        "model_select = widgets.Dropdown(options=['t5-small','facebook/bart-base','google/flan-t5-small'], value='t5-small', description='Modelo:')\n",
        "epochs = widgets.IntText(value=3, description='Épocas:')\n",
        "batch_size = widgets.IntText(value=4, description='Batch Size:')\n",
        "lr = widgets.FloatText(value=5e-5, description='Learning Rate:')\n",
        "start_btn = widgets.Button(description='▶️ Iniciar Fine-Tuning', button_style='success')\n",
        "output = widgets.Output()\n",
        "\n",
        "def run_finetune(b):\n",
        "    with output:\n",
        "        clear_output()\n",
        "        if not train_upload.value or not valid_upload.value:\n",
        "            print(\"⚠️ Faça upload de train.json e valid.json.\")\n",
        "            return\n",
        "        # Load JSON\n",
        "        train_data = json.load(io.BytesIO(next(iter(train_upload.value.values()))['content']))\n",
        "        valid_data = json.load(io.BytesIO(next(iter(valid_upload.value.values()))['content']))\n",
        "        # Create datasets\n",
        "        def to_dataset(data):\n",
        "            return Dataset.from_dict({\n",
        "                'input_text': [f\"cenario: {v['scenario']}\" for v in data.values()],\n",
        "                'target_text': [v['diagnosis'] for v in data.values()]\n",
        "            })\n",
        "        train_ds = to_dataset(train_data)\n",
        "        valid_ds = to_dataset(valid_data)\n",
        "        # Tokenizer and model\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_select.value)\n",
        "        model = AutoModelForSeq2SeqLM.from_pretrained(model_select.value)\n",
        "        # Preprocess\n",
        "        def preprocess(batch):\n",
        "            inputs = tokenizer(batch['input_text'], padding='max_length', truncation=True, max_length=256)\n",
        "            labels = tokenizer(batch['target_text'], padding='max_length', truncation=True, max_length=64).input_ids\n",
        "            inputs['labels'] = labels\n",
        "            return inputs\n",
        "        train_tok = train_ds.map(preprocess, batched=True)\n",
        "        valid_tok = valid_ds.map(preprocess, batched=True)\n",
        "        data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
        "        # TrainingArguments\n",
        "        args = TrainingArguments(\n",
        "            output_dir='ft_basic',\n",
        "            num_train_epochs=epochs.value,\n",
        "            per_device_train_batch_size=batch_size.value,\n",
        "            per_device_eval_batch_size=batch_size.value,\n",
        "            learning_rate=lr.value,\n",
        "            evaluation_strategy='steps',\n",
        "            eval_steps=50,\n",
        "            save_total_limit=2,\n",
        "            logging_steps=20,\n",
        "            load_best_model_at_end=True\n",
        "        )\n",
        "        trainer = Trainer(\n",
        "            model=model,\n",
        "            args=args,\n",
        "            train_dataset=train_tok,\n",
        "            eval_dataset=valid_tok,\n",
        "            data_collator=data_collator,\n",
        "            tokenizer=tokenizer\n",
        "        )\n",
        "        print(\"🔄 Treinamento iniciado...\")\n",
        "        trainer.train()\n",
        "        trainer.save_model('ft_basic_model')\n",
        "        print(\"✅ Fine-tuning completado. Modelo salvo em 'ft_basic_model'.\")\n",
        "\n",
        "start_btn.on_click(run_finetune)\n",
        "display(widgets.VBox([train_upload, valid_upload, model_select, epochs, batch_size, lr, start_btn, output]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b30054b4",
      "metadata": {
        "id": "b30054b4"
      },
      "outputs": [],
      "source": [
        "# Instalação de dependências para augmentation e adversarial\n",
        "%pip install nlpaug textattack transformers ipywidgets --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08e15191",
      "metadata": {
        "id": "08e15191"
      },
      "outputs": [],
      "source": [
        "import nlpaug.augmenter.word as naw\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "# Widgets for augmentation\n",
        "sample_text = widgets.Textarea(value='Paciente com dor abdominal intensa e febre.', description='Texto:')\n",
        "methods = widgets.SelectMultiple(\n",
        "    options=['back_translation', 'synonym', 'random_deletion'],\n",
        "    value=['synonym'],\n",
        "    description='Métodos:'\n",
        ")\n",
        "bt_src = widgets.Dropdown(options=['en', 'pt'], value='pt', description='BackTrans Src:')\n",
        "bt_mid = widgets.Dropdown(options=['en'], value='en', description='BackTrans Mid:')\n",
        "syn_model = widgets.Dropdown(options=['wordnet', 'contextual'], value='wordnet', description='Syn Model:')\n",
        "del_prob = widgets.FloatSlider(value=0.1, min=0.01, max=0.5, step=0.01, description='Del Prob:')\n",
        "aug_btn = widgets.Button(description='▶️ Augment', button_style='success')\n",
        "out_aug = widgets.Output()\n",
        "\n",
        "def augment_text(b):\n",
        "    with out_aug:\n",
        "        clear_output()\n",
        "        text = sample_text.value\n",
        "        aug_texts = {}\n",
        "        for m in methods.value:\n",
        "            if m == 'back_translation':\n",
        "                bt = naw.BackTranslationAug(src_lang=bt_src.value, mid_lang=bt_mid.value)\n",
        "                aug_texts['back_translation'] = bt.augment(text)\n",
        "            elif m == 'synonym':\n",
        "                if syn_model.value == 'wordnet':\n",
        "                    syn = naw.SynonymAug(aug_src='wordnet')\n",
        "                else:\n",
        "                    syn = naw.ContextualWordEmbsAug(model_path='bert-base-uncased', action=\"substitute\")\n",
        "                aug_texts['synonym'] = syn.augment(text)\n",
        "            elif m == 'random_deletion':\n",
        "                rd = naw.RandomWordAug(action=\"delete\", aug_p=del_prob.value)\n",
        "                aug_texts['random_deletion'] = rd.augment(text)\n",
        "        for k, v in aug_texts.items():\n",
        "            print(f\"--- {k} ---\\n{v}\\n\")\n",
        "\n",
        "augment_text(aug_btn)\n",
        "display(widgets.VBox([sample_text, methods,\n",
        "                     widgets.HBox([bt_src, bt_mid, syn_model, del_prob]),\n",
        "                     aug_btn, out_aug]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf0cbacf",
      "metadata": {
        "id": "bf0cbacf"
      },
      "outputs": [],
      "source": [
        "# Adversarial testing with TextAttack\n",
        "import textattack\n",
        "from textattack.attack_recipes import TextFoolerJin2019, DeepWordBugGao2018\n",
        "from transformers import pipeline\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "# Widgets for adversarial\n",
        "attack_method = widgets.Dropdown(\n",
        "    options=['TextFooler', 'DeepWordBug'],\n",
        "    value='TextFooler',\n",
        "    description='Ataque:'\n",
        ")\n",
        "model_select = widgets.Dropdown(\n",
        "    options=['bert-base-uncased', 'roberta-base'],\n",
        "    value='bert-base-uncased',\n",
        "    description='Modelo:'\n",
        ")\n",
        "adv_input = widgets.Textarea(value='Paciente apresenta tosse seca persistente.', description='Input:')\n",
        "adv_btn = widgets.Button(description='▶️ Attack', button_style='danger')\n",
        "out_adv = widgets.Output()\n",
        "\n",
        "def run_attack(b):\n",
        "    with out_adv:\n",
        "        clear_output()\n",
        "        # Build model\n",
        "        model = textattack.models.wrappers.HuggingFaceModelWrapper(\n",
        "            pipeline('text-classification', model=model_select.value, return_all_scores=False, device=-1).model,\n",
        "            pipeline('text-classification', model=model_select.value, return_all_scores=False, device=-1).tokenizer\n",
        "        )\n",
        "        # Choose attack\n",
        "        if attack_method.value == 'TextFooler':\n",
        "            attack = TextFoolerJin2019.build(model)\n",
        "        else:\n",
        "            attack = DeepWordBugGao2018.build(model)\n",
        "        # Attack\n",
        "        text_attack = textattack.Attacker(attack, textattack.datasets.Dataset([[adv_input.value, None]]))\n",
        "        result = text_attack.attack_dataset(num_examples=1)\n",
        "        for r in result:\n",
        "            print(r)\n",
        "\n",
        "adv_btn.on_click(run_attack)\n",
        "display(widgets.VBox([attack_method, model_select, adv_input, adv_btn, out_adv]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05af58f6",
      "metadata": {
        "id": "05af58f6"
      },
      "outputs": [],
      "source": [
        "# Instalação de dependências para EDA avançada\n",
        "%pip install pandas plotly nltk spacy wordcloud ipywidgets --quiet\n",
        "\n",
        "import io\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import spacy\n",
        "from wordcloud import WordCloud\n",
        "import plotly.express as px\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output, HTML\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "from nltk.corpus import stopwords\n",
        "stopwords_pt = set(stopwords.words('portuguese'))\n",
        "\n",
        "# Load spaCy model for entity recognition\n",
        "try:\n",
        "    nlp = spacy.load('pt_core_news_sm')\n",
        "except:\n",
        "    spacy.cli.download('pt_core_news_sm')\n",
        "    nlp = spacy.load('pt_core_news_sm')\n",
        "\n",
        "# Widgets\n",
        "upload = widgets.FileUpload(accept='.csv,.txt', multiple=False, description='Upload Corpus')\n",
        "text_col = widgets.Text(value='text', description='Coluna (CSV):')\n",
        "top_k_terms = widgets.IntSlider(value=20, min=5, max=100, step=5, description='Top-K Termos:')\n",
        "top_k_bi = widgets.IntSlider(value=10, min=5, max=50, step=5, description='Top-K Bigramas:')\n",
        "run_btn = widgets.Button(description='▶️ Executar EDA', button_style='success')\n",
        "out = widgets.Output()\n",
        "\n",
        "def run_eda(b):\n",
        "    with out:\n",
        "        clear_output()\n",
        "        # Load text data\n",
        "        texts = []\n",
        "        if upload.value:\n",
        "            name, info = next(iter(upload.value.items()))\n",
        "            content = info['content']\n",
        "            if name.lower().endswith('.csv'):\n",
        "                df = pd.read_csv(io.BytesIO(content))\n",
        "                if text_col.value in df.columns:\n",
        "                    texts = df[text_col.value].astype(str).tolist()\n",
        "                else:\n",
        "                    print(f\"Coluna '{text_col.value}' não encontrada.\")\n",
        "                    return\n",
        "            else:\n",
        "                texts = content.decode('utf-8').splitlines()\n",
        "        else:\n",
        "            print(\"Faça upload de um arquivo\")\n",
        "            return\n",
        "\n",
        "        # Compute lengths\n",
        "        lengths = [len(nltk.word_tokenize(t)) for t in texts]\n",
        "        fig1 = px.histogram(lengths, nbins=30, title='Distribuição de Comprimento (tokens)',\n",
        "                            labels={'value':'Tokens'}, opacity=0.75)\n",
        "        fig1.show()\n",
        "\n",
        "        # Term frequency\n",
        "        all_tokens = [tok.lower() for t in texts for tok in nltk.word_tokenize(t) if tok.isalpha() and tok.lower() not in stopwords_pt]\n",
        "        freq = pd.Series(all_tokens).value_counts().head(top_k_terms.value).reset_index()\n",
        "        freq.columns = ['term','count']\n",
        "        fig2 = px.bar(freq, x='term', y='count', title=f'Top {top_k_terms.value} Termos')\n",
        "        fig2.show()\n",
        "\n",
        "        # Bigrams\n",
        "        bigrams = list(nltk.bigrams(all_tokens))\n",
        "        bi_freq = pd.Series([' '.join(b) for b in bigrams]).value_counts().head(top_k_bi.value).reset_index()\n",
        "        bi_freq.columns = ['bigram','count']\n",
        "        fig3 = px.bar(bi_freq, x='bigram', y='count', title=f'Top {top_k_bi.value} Bigramas')\n",
        "        fig3.show()\n",
        "\n",
        "        # Wordcloud\n",
        "        wc = WordCloud(width=800, height=400, background_color='white').generate(' '.join(all_tokens))\n",
        "        display(HTML(f\"<img src='{wc.to_image()._repr_png_()}'/>\"))\n",
        "\n",
        "        # Entity stats\n",
        "        docs = [nlp(t) for t in texts]\n",
        "        ents = [ent.label_ for doc in docs for ent in doc.ents]\n",
        "        ent_freq = pd.Series(ents).value_counts().reset_index()\n",
        "        ent_freq.columns = ['entity','count']\n",
        "        fig4 = px.bar(ent_freq, x='entity', y='count', title='Frequência de Entidades Médicas')\n",
        "        fig4.show()\n",
        "\n",
        "run_btn.on_click(run_eda)\n",
        "display(widgets.VBox([upload, text_col, top_k_terms, top_k_bi, run_btn, out]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6363833",
      "metadata": {
        "id": "c6363833"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "from sklearn.decomposition import PCA\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "# Widgets to select providers and visualization type\n",
        "providers = widgets.SelectMultiple(\n",
        "    options=['HuggingFace', 'OpenAI', 'Vertex'],\n",
        "    value=['HuggingFace', 'OpenAI', 'Vertex'],\n",
        "    description='Provedores:'\n",
        ")\n",
        "viz_type = widgets.RadioButtons(\n",
        "    options=['Heatmap', 'Histograma', 'PCA Scatter'],\n",
        "    value='Heatmap',\n",
        "    description='Visualizar:'\n",
        ")\n",
        "run_btn = widgets.Button(description='▶️ Plotar', button_style='info')\n",
        "out = widgets.Output()\n",
        "\n",
        "# Assume embeddings stored in dict: all_embs[prov] from Section 4a\n",
        "# For demonstration, regenerate minimal embeddings if needed.\n",
        "\n",
        "def plot_viz(b):\n",
        "    with out:\n",
        "        clear_output()\n",
        "        selected = providers.value\n",
        "        viz = viz_type.value\n",
        "        if not selected:\n",
        "            print(\"Selecione ao menos um provedor.\")\n",
        "            return\n",
        "        # Load or access embeddings stored in notebook global\n",
        "        try:\n",
        "            embs = {prov: globals()['all_embs'][f\"{prov}:{''}\"] for prov in selected}\n",
        "        except:\n",
        "            print(\"Embeddings não encontrados. Execute a seção 4a primeiro.\")\n",
        "            return\n",
        "        # Use first 100 items for speed\n",
        "        embs = {prov: emb[:100] for prov, emb in embs.items()}\n",
        "        # Combine into DataFrame\n",
        "        df_list = []\n",
        "        for prov, emb in embs.items():\n",
        "            for vec in emb:\n",
        "                df_list.append({'provider': prov, **{f\"dim{i}\": vec[i] for i in range(min(10, vec.shape[0]))}})\n",
        "        df = pd.DataFrame(df_list)\n",
        "        if viz == 'Heatmap':\n",
        "            # Compute similarity matrix per provider on first text\n",
        "            sim_mats = {}\n",
        "            for prov, emb in embs.items():\n",
        "                sim = cosine_similarity(emb)\n",
        "                sim_mats[prov] = sim\n",
        "                fig = px.imshow(sim, title=f\"Heatmap {prov}\", labels={'x':'Index','y':'Index'})\n",
        "                fig.show()\n",
        "        elif viz == 'Histograma':\n",
        "            for prov, emb in embs.items():\n",
        "                sims = cosine_similarity(emb)\n",
        "                # take upper triangle excluding diag\n",
        "                vals = sims[np.triu_indices_from(sims, k=1)]\n",
        "                fig = px.histogram(vals, nbins=50, title=f\"Histograma de Similaridade {prov}\", labels={'value':'Similaridade'})\n",
        "                fig.show()\n",
        "        else:  # PCA Scatter\n",
        "            pca = PCA(n_components=2)\n",
        "            combined = np.vstack(list(embs.values()))\n",
        "            proj = pca.fit_transform(combined)\n",
        "            labels = sum([[prov]*embs[prov].shape[0] for prov in embs], [])\n",
        "            df_pca = pd.DataFrame({'pca1': proj[:,0], 'pca2': proj[:,1], 'provider': labels})\n",
        "            fig = px.scatter(df_pca, x='pca1', y='pca2', color='provider', title=\"PCA Scatter of Embeddings\")\n",
        "            fig.show()\n",
        "\n",
        "run_btn.on_click(plot_viz)\n",
        "display(widgets.VBox([providers, viz_type, run_btn, out]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a0ec206",
      "metadata": {
        "id": "2a0ec206"
      },
      "outputs": [],
      "source": [
        "# Instalação de dependências para embeddings multimodelo\n",
        "%pip install transformers torch openai google-cloud-aiplatform pandas scikit-learn plotly ipywidgets --quiet\n",
        "\n",
        "import os\n",
        "import io\n",
        "import openai\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output, HTML\n",
        "import plotly.express as px\n",
        "from google.cloud import aiplatform\n",
        "\n",
        "# Configure API keys\n",
        "openai.api_key = os.getenv('OPENAI_API_KEY', '')\n",
        "VERTEX_KEY = os.getenv('VERTEX_API_KEY', '')\n",
        "\n",
        "# Widgets\n",
        "upload = widgets.FileUpload(accept='.csv,.txt', multiple=False, description='Upload Corpus')\n",
        "text_col = widgets.Text(value='text', description='Coluna (CSV):')\n",
        "manual_input = widgets.Textarea(description='Input Manual:', layout=widgets.Layout(width='100%',height='80px'))\n",
        "providers = widgets.SelectMultiple(\n",
        "    options=['HuggingFace:all-MiniLM-L6-v2', 'HuggingFace:biobert-base-cased-v1.1',\n",
        "             'OpenAI:text-embedding-ada-002', 'Vertex:textembedding-gecko'],\n",
        "    value=['HuggingFace:all-MiniLM-L6-v2'], description='Provedores:'\n",
        ")\n",
        "batch_size = widgets.IntSlider(value=16, min=1, max=128, step=1, description='Batch Size:')\n",
        "run_btn = widgets.Button(description='Gerar Embeddings', button_style='success')\n",
        "out = widgets.Output()\n",
        "\n",
        "def load_corpus():\n",
        "    texts = []\n",
        "    if upload.value:\n",
        "        fname, dat = next(iter(upload.value.items()))\n",
        "        content = dat['content']\n",
        "        if fname.endswith('.csv'):\n",
        "            df = pd.read_csv(io.BytesIO(content))\n",
        "            if text_col.value in df.columns:\n",
        "                texts = df[text_col.value].astype(str).tolist()\n",
        "        else:\n",
        "            lines = content.decode('utf-8').splitlines()\n",
        "            texts = [l for l in lines if l.strip()]\n",
        "    # manual input overrides\n",
        "    if manual_input.value.strip():\n",
        "        texts = [s for s in manual_input.value.splitlines() if s.strip()]\n",
        "    return texts\n",
        "\n",
        "def get_hf_embeddings(model_name, texts, batch=16):\n",
        "    tok = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModel.from_pretrained(model_name)\n",
        "    model.eval()\n",
        "    embs = []\n",
        "    for i in range(0, len(texts), batch):\n",
        "        batch_text = texts[i:i+batch]\n",
        "        inputs = tok(batch_text, return_tensors='pt', padding=True, truncation=True)\n",
        "        with torch.no_grad():\n",
        "            outp = model(**inputs)\n",
        "            emb = outp.last_hidden_state.mean(dim=1).numpy()\n",
        "        embs.append(emb)\n",
        "    return np.vstack(embs)\n",
        "\n",
        "def get_openai_embeddings(texts, batch=16):\n",
        "    embs = []\n",
        "    for i in range(0, len(texts), batch):\n",
        "        resp = openai.Embedding.create(input=texts[i:i+batch], model='text-embedding-ada-002')\n",
        "        embs.extend([np.array(d['embedding']) for d in resp['data']])\n",
        "    return np.vstack(embs)\n",
        "\n",
        "def get_vertex_embeddings(texts, batch=16):\n",
        "    client = aiplatform.gapic.PredictionServiceClient()\n",
        "    embs = []\n",
        "    for i in range(0, len(texts), batch):\n",
        "        inst = [{\"content\": t} for t in texts[i:i+batch]]\n",
        "        endpoint = os.getenv('VERTEX_ENDPOINT', '')\n",
        "        response = client.predict(endpoint=endpoint, instances=inst, parameters={})\n",
        "        for pr in response.predictions:\n",
        "            embs.append(np.array(pr.get('embeddings', pr.get('content', []))))\n",
        "    return np.vstack(embs)\n",
        "\n",
        "def run_embeddings(b):\n",
        "    with out:\n",
        "        clear_output()\n",
        "        texts = load_corpus()\n",
        "        if not texts:\n",
        "            print(\"⚠️ Forneça corpus ou input manual.\")\n",
        "            return\n",
        "        all_embs = {}\n",
        "        for prov in providers.value:\n",
        "            name, model = prov.split(':',1)\n",
        "            print(f\"Gerando embeddings com {prov}...\")\n",
        "            if name=='HuggingFace':\n",
        "                emb = get_hf_embeddings(model, texts, batch_size.value)\n",
        "            elif name=='OpenAI':\n",
        "                emb = get_openai_embeddings(texts, batch_size.value)\n",
        "            else:\n",
        "                emb = get_vertex_embeddings(texts, batch_size.value)\n",
        "            all_embs[prov] = emb\n",
        "        # Compare similarity for first text\n",
        "        sim_df = pd.DataFrame()\n",
        "        for prov, emb in all_embs.items():\n",
        "            sims = cosine_similarity(emb[0:1], emb)[0]\n",
        "            sim_df[prov] = sims\n",
        "        # Display similarity table\n",
        "        display(sim_df.head())\n",
        "        # Visualization: bar of top-5 similar per provider\n",
        "        for prov in sim_df.columns:\n",
        "            top_idxs = np.argsort(sim_df[prov])[-5:][::-1]\n",
        "            fig = px.bar(x=[texts[i] for i in top_idxs], y=sim_df.loc[top_idxs, prov],\n",
        "                         title=f\"Top 5 similares by {prov}\", labels={'x':'Texto', 'y':'Similaridade'})\n",
        "            fig.show()\n",
        "\n",
        "run_btn.on_click(run_embeddings)\n",
        "display(widgets.VBox([upload, text_col, manual_input, providers, batch_size, run_btn, out]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c78a2d99",
      "metadata": {
        "id": "c78a2d99"
      },
      "outputs": [],
      "source": [
        "# Instalação de dependências\n",
        "%pip install datasets numpy pandas ipywidgets --quiet\n",
        "\n",
        "import io\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datasets import Dataset\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "# Upload widgets for modalities\n",
        "text_upload = widgets.FileUpload(accept='.txt,.csv', multiple=False, description='Texto Clínico')\n",
        "audio_upload = widgets.FileUpload(accept='.json', multiple=False, description='Áudio (JSON)')\n",
        "image_upload = widgets.FileUpload(accept='.csv,.json', multiple=False, description='Imagens (CSV/JSON)')\n",
        "fhir_upload = widgets.FileUpload(accept='.json', multiple=False, description='FHIR JSON')\n",
        "# Split ratio widgets\n",
        "val_ratio = widgets.FloatSlider(value=0.1, min=0.0, max=0.4, step=0.05, description='Val Ratio')\n",
        "test_ratio = widgets.FloatSlider(value=0.1, min=0.0, max=0.4, step=0.05, description='Test Ratio')\n",
        "build_btn = widgets.Button(description='▶️ Construir Dataset', button_style='success')\n",
        "out = widgets.Output()\n",
        "\n",
        "def build_dataset(b):\n",
        "    with out:\n",
        "        clear_output()\n",
        "        # Load text data\n",
        "        records = {}\n",
        "        rec_id = 0\n",
        "        if text_upload.value:\n",
        "            name, info = next(iter(text_upload.value.items()))\n",
        "            content = info['content']\n",
        "            if name.lower().endswith('.txt'):\n",
        "                lines = content.decode('utf-8').splitlines()\n",
        "                for line in lines:\n",
        "                    records[str(rec_id)] = {'text': line}\n",
        "                    rec_id += 1\n",
        "            else:\n",
        "                df = pd.read_csv(io.BytesIO(content))\n",
        "                for _, row in df.iterrows():\n",
        "                    records[str(rec_id)] = {'text': row.iloc[0]}\n",
        "                    rec_id += 1\n",
        "        # Load audio transcripts\n",
        "        if audio_upload.value:\n",
        "            name, info = next(iter(audio_upload.value.items()))\n",
        "            audio_data = json.loads(info['content'].decode('utf-8'))\n",
        "            for entry in audio_data:\n",
        "                records[str(rec_id)] = {'audio_transcript': entry.get('text', '')}\n",
        "                rec_id += 1\n",
        "        # Load image captions\n",
        "        if image_upload.value:\n",
        "            name, info = next(iter(image_upload.value.items()))\n",
        "            if name.lower().endswith('.csv'):\n",
        "                df = pd.read_csv(io.BytesIO(info['content']))\n",
        "                for _, row in df.iterrows():\n",
        "                    records[str(rec_id)] = {'image_caption': row.iloc[0]}\n",
        "                    rec_id += 1\n",
        "            else:\n",
        "                img_json = json.loads(info['content'].decode('utf-8'))\n",
        "                for item in img_json:\n",
        "                    records[str(rec_id)] = {'image_caption': item.get('caption', '')}\n",
        "                    rec_id += 1\n",
        "        # Load FHIR resources\n",
        "        if fhir_upload.value:\n",
        "            for name, info in fhir_upload.value.items():\n",
        "                fhir_data = json.loads(info['content'].decode('utf-8'))\n",
        "                records[str(rec_id)] = {'fhir_resource': fhir_data}\n",
        "                rec_id += 1\n",
        "        if not records:\n",
        "            print(\"Nenhum dado carregado. Faça upload de pelo menos uma modalidade.\")\n",
        "            return\n",
        "        df = pd.DataFrame.from_dict(records, orient='index')\n",
        "        # Shuffle and split\n",
        "        ratios = (1 - val_ratio.value - test_ratio.value, val_ratio.value, test_ratio.value)\n",
        "        if ratios[0] <= 0:\n",
        "            print(\"Val+Test Ratio deve ser < 1.\")\n",
        "            return\n",
        "        df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "        n = len(df)\n",
        "        train_end = int(ratios[0]*n)\n",
        "        val_end = train_end + int(ratios[1]*n)\n",
        "        splits = {\n",
        "            'train': df.iloc[:train_end],\n",
        "            'valid': df.iloc[train_end:val_end],\n",
        "            'test': df.iloc[val_end:]\n",
        "        }\n",
        "        # Save JSON splits\n",
        "        for split, subset in splits.items():\n",
        "            out_dict = subset.to_dict(orient='index')\n",
        "            with open(f\"{split}.json\", 'w', encoding='utf-8') as f:\n",
        "                json.dump(out_dict, f, ensure_ascii=False, indent=2)\n",
        "            print(f\"{split}.json salvo com {len(subset)} registros.\")\n",
        "        # Integrity tests\n",
        "        all_ids = sum([list(splits[s].index.astype(str)) for s in splits], [])\n",
        "        if len(all_ids) == len(set(all_ids)):\n",
        "            print(\"✔ IDs únicos em todos splits.\")\n",
        "        else:\n",
        "            print(\"⚠ IDs duplicados detectados!\")\n",
        "        # Mandatory fields check\n",
        "        print(\"Campos disponíveis:\", list(df.columns))\n",
        "        print(\"Dataset construído com sucesso.\")\n",
        "\n",
        "build_btn.on_click(build_dataset)\n",
        "display(widgets.VBox([text_upload, audio_upload, image_upload, fhir_upload,\n",
        "                      val_ratio, test_ratio, build_btn, out]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3aa9800a",
      "metadata": {
        "id": "3aa9800a"
      },
      "outputs": [],
      "source": [
        "# Instalar dependências necessárias\n",
        "%pip install spacy ipywidgets flashtext python-dateutil --quiet\n",
        "\n",
        "import io\n",
        "import re\n",
        "import json\n",
        "import ipywidgets as widgets\n",
        "from flashtext import KeywordProcessor\n",
        "import spacy\n",
        "from dateutil import parser as dateparser\n",
        "from IPython.display import display, clear_output, Markdown\n",
        "\n",
        "# Carregar spaCy (modelo português)\n",
        "try:\n",
        "    nlp_deid = spacy.load('pt_core_news_sm')\n",
        "except:\n",
        "    spacy.cli.download('pt_core_news_sm')\n",
        "    nlp_deid = spacy.load('pt_core_news_sm')\n",
        "\n",
        "# Acronym processor\n",
        "acro_input = widgets.Textarea(\n",
        "    placeholder='Ex: UTI: Unidade de Terapia Intensiva\\nSARA: Síndrome Respiratória Aguda Grave',\n",
        "    description='Acrônimos:', layout=widgets.Layout(width='100%',height='80px')\n",
        ")\n",
        "file_upload = widgets.FileUpload(accept='.txt,.csv', multiple=False, description='Upload Texto')\n",
        "mask_phi_cb = widgets.Checkbox(value=True, description='Remover PHI')\n",
        "expand_acro_cb = widgets.Checkbox(value=False, description='Expandir Acrônimos')\n",
        "highlight_ent_cb = widgets.Checkbox(value=False, description='Destacar Entidades')\n",
        "process_btn = widgets.Button(description='Processar Texto', button_style='success')\n",
        "output = widgets.Output()\n",
        "\n",
        "def deidentify_text(text):\n",
        "    # Mask dates\n",
        "    text = re.sub(r'\\b\\d{1,2}/\\d{1,2}/\\d{2,4}\\b', '[DATA]', text)\n",
        "    # Mask emails\n",
        "    text = re.sub(r'\\S+@\\S+', '[EMAIL]', text)\n",
        "    # Mask phone numbers\n",
        "    text = re.sub(r'\\b\\(?\\d{2,3}\\)?[\\s-]?\\d{4,5}-?\\d{4}\\b', '[TEL]', text)\n",
        "    # Mask names with NER\n",
        "    doc = nlp_deid(text)\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ in ('PER', 'PROPN'):\n",
        "            text = text.replace(ent.text, '[NOME]')\n",
        "    return text\n",
        "\n",
        "def expand_acronyms(text, mapping):\n",
        "    processor = KeywordProcessor()\n",
        "    for k,v in mapping.items():\n",
        "        processor.add_keyword(k, v)\n",
        "    return processor.replace_keywords(text)\n",
        "\n",
        "def highlight_entities(text):\n",
        "    doc = nlp_deid(text)\n",
        "    spans = []\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ in ('DISEASE', 'DRUG', 'ORG'):\n",
        "            spans.append(f\"[{ent.text}|{ent.label_}]\")\n",
        "    return ' '.join(spans)\n",
        "\n",
        "def on_process(b):\n",
        "    with output:\n",
        "        clear_output()\n",
        "        if not file_upload.value:\n",
        "            print(\"Faça upload de um arquivo de texto.\")\n",
        "            return\n",
        "        # Read content\n",
        "        fname, info = next(iter(file_upload.value.items()))\n",
        "        content = info['content'].decode('utf-8')\n",
        "        text = content\n",
        "        # Build acronym mapping\n",
        "        mapping = {}\n",
        "        if expand_acro_cb.value and acro_input.value.strip():\n",
        "            for line in acro_input.value.splitlines():\n",
        "                if ':' in line:\n",
        "                    k,v = line.split(':',1)\n",
        "                    mapping[k.strip()] = v.strip()\n",
        "        # Apply PHI masking\n",
        "        if mask_phi_cb.value:\n",
        "            text = deidentify_text(text)\n",
        "        # Expand acronyms\n",
        "        if expand_acro_cb.value:\n",
        "            text = expand_acronyms(text, mapping)\n",
        "        # Highlight entities\n",
        "        if highlight_ent_cb.value:\n",
        "            ents = highlight_entities(text)\n",
        "            display(Markdown(f\"**Entidades Médicas:** {ents}\"))\n",
        "        print(\"Texto Processado:\\n\")\n",
        "        print(text[:1000] + ('...' if len(text)>1000 else ''))\n",
        "        # Save processed text\n",
        "        out_file = 'text_desidentified.txt'\n",
        "        with open(out_file, 'w', encoding='utf-8') as f:\n",
        "            f.write(text)\n",
        "        print(f\"✅ Processado salvo em {out_file}\")\n",
        "\n",
        "process_btn.on_click(on_process)\n",
        "display(widgets.VBox([\n",
        "    file_upload, mask_phi_cb, acro_input, expand_acro_cb, highlight_ent_cb, process_btn, output\n",
        "]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "386f1794",
      "metadata": {
        "id": "386f1794"
      },
      "outputs": [],
      "source": [
        "# Instalação de dependências para pré-processamento avançado\n",
        "%pip install nltk spacy joblib diskcache ipywidgets --quiet\n",
        "\n",
        "import io\n",
        "import os\n",
        "import pandas as pd\n",
        "from diskcache import Cache\n",
        "from joblib import Parallel, delayed\n",
        "import nltk\n",
        "import spacy\n",
        "import re\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "# Download resources\n",
        "nltk.download('stopwords', quiet=True)\n",
        "from nltk.corpus import stopwords\n",
        "stopwords_pt = set(stopwords.words('portuguese'))\n",
        "\n",
        "# Load spaCy model\n",
        "try:\n",
        "    nlp = spacy.load('pt_core_news_sm')\n",
        "except:\n",
        "    spacy.cli.download('pt_core_news_sm')\n",
        "    nlp = spacy.load('pt_core_news_sm')\n",
        "\n",
        "# Widgets for configuration\n",
        "upload_corpus = widgets.FileUpload(accept='.txt,.csv', multiple=False, description='Upload Corpus')\n",
        "workers = widgets.IntSlider(value=4, min=1, max=16, description='Workers:')\n",
        "use_cache = widgets.Checkbox(value=True, description='Habilitar Cache')\n",
        "steps = widgets.SelectMultiple(\n",
        "    options=['lowercase','remove_specials','remove_stopwords','lemmatize'],\n",
        "    value=['lowercase','remove_specials','remove_stopwords'],\n",
        "    description='Steps:'\n",
        ")\n",
        "process_btn = widgets.Button(description='▶️ Processar Corpus', button_style='success')\n",
        "out = widgets.Output()\n",
        "\n",
        "# Initialize cache\n",
        "cache = Cache(directory='preproc_cache')\n",
        "\n",
        "def preprocess_text(text, steps):\n",
        "    # check cache\n",
        "    key = (text[:50], tuple(steps))\n",
        "    if use_cache.value and key in cache:\n",
        "        return cache[key]\n",
        "    t = text\n",
        "    if 'lowercase' in steps:\n",
        "        t = t.lower()\n",
        "    if 'remove_specials' in steps:\n",
        "        t = re.sub(r'[^\\w\\s]', ' ', t)\n",
        "    tokens = t.split()\n",
        "    if 'remove_stopwords' in steps:\n",
        "        tokens = [w for w in tokens if w not in stopwords_pt]\n",
        "    if 'lemmatize' in steps:\n",
        "        doc = nlp(' '.join(tokens))\n",
        "        tokens = [token.lemma_ for token in doc]\n",
        "    result = ' '.join(tokens)\n",
        "    if use_cache.value:\n",
        "        cache[key] = result\n",
        "    return result\n",
        "\n",
        "def process_corpus(b):\n",
        "    with out:\n",
        "        clear_output()\n",
        "        if not upload_corpus.value:\n",
        "            print(\"Faça upload de um corpus TXT ou CSV.\")\n",
        "            return\n",
        "        fname, info = next(iter(upload_corpus.value.items()))\n",
        "        content = info['content']\n",
        "        ext = fname.split('.')[-1].lower()\n",
        "        if ext == 'txt':\n",
        "            data = content.decode('utf-8').splitlines()\n",
        "        else:\n",
        "            df = pd.read_csv(io.BytesIO(content))\n",
        "            if 'text' not in df.columns:\n",
        "                print(\"CSV deve conter coluna 'text'.\")\n",
        "                return\n",
        "            data = df['text'].astype(str).tolist()\n",
        "        print(f\"Processando {len(data)} linhas com {workers.value} workers...\")\n",
        "        # Parallel processing\n",
        "        processed = Parallel(n_jobs=workers.value)(\n",
        "            delayed(preprocess_text)(line, steps.value) for line in data\n",
        "        )\n",
        "        # Display sample and save\n",
        "        print(\"Amostra processada:\")\n",
        "        for i, line in enumerate(processed[:5]):\n",
        "            print(f\"{i+1}: {line[:100]}...\")\n",
        "        # Save results\n",
        "        out_file = 'processed_corpus.txt'\n",
        "        with open(out_file, 'w', encoding='utf-8') as f:\n",
        "            for line in processed:\n",
        "                f.write(line + '\\n')\n",
        "        print(f\"Corpus processado salvo em {out_file}\")\n",
        "\n",
        "process_btn.on_click(process_corpus)\n",
        "display(widgets.VBox([upload_corpus, workers, use_cache, steps, process_btn, out]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06b10d5d",
      "metadata": {
        "id": "06b10d5d"
      },
      "outputs": [],
      "source": [
        "# Instalação de dependências avançadas\n",
        "%pip install pdfplumber fhir.resources aiohttp beautifulsoup4 tqdm ipywidgets --quiet\n",
        "\n",
        "import io, os, json, asyncio\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import pdfplumber\n",
        "import aiohttp\n",
        "from bs4 import BeautifulSoup\n",
        "from fhir.resources import construct_fhir_element\n",
        "from tqdm.auto import tqdm\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output, Markdown\n",
        "\n",
        "# Widgets\n",
        "pdf_upload = widgets.FileUpload(accept='.pdf', multiple=False, description='PDF')\n",
        "max_pages = widgets.IntText(value=10000, description='Max Pages:')\n",
        "chunk_size = widgets.IntText(value=1000, description='Chunk Size:')\n",
        "workers = widgets.IntSlider(value=4, min=1, max=16, description='Workers:')\n",
        "fhir_upload = widgets.FileUpload(accept='.json', multiple=True, description='FHIR JSONs')\n",
        "url_list = widgets.Textarea(placeholder='One URL per line...', description='URLs:', layout=widgets.Layout(width='100%', height='80px'))\n",
        "run_btn = widgets.Button(description='Processar Ingestão', button_style='success')\n",
        "out = widgets.Output()\n",
        "\n",
        "async def scrape_url(session, url):\n",
        "    try:\n",
        "        async with session.get(url, timeout=10) as resp:\n",
        "            html = await resp.text()\n",
        "        soup = BeautifulSoup(html, 'html.parser')\n",
        "        return url, {\n",
        "            'titles': [h.get_text(strip=True) for h in soup.find_all(['h1','h2'])][:5],\n",
        "            'paragraphs': [p.get_text(strip=True)[:200] for p in soup.find_all('p')][:5]\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return url, {'error': str(e)}\n",
        "\n",
        "async def perform_scraping(urls):\n",
        "    results = {}\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "        tasks = [scrape_url(session, u) for u in urls]\n",
        "        for coro in asyncio.as_completed(tasks):\n",
        "            url, data = await coro\n",
        "            results[url] = data\n",
        "    return results\n",
        "\n",
        "def process_all(b):\n",
        "    with out:\n",
        "        clear_output()\n",
        "        # PDF processing\n",
        "        if pdf_upload.value:\n",
        "            fname, info = next(iter(pdf_upload.value.items()))\n",
        "            content = info['content']\n",
        "            text_chunks = []\n",
        "            with pdfplumber.open(io.BytesIO(content)) as pdf:\n",
        "                total = min(len(pdf.pages), max_pages.value)\n",
        "                pages = range(total)\n",
        "                # process in threads\n",
        "                def extract_range(start):\n",
        "                    txt = ''\n",
        "                    for i in range(start, min(start+chunk_size.value, total)):\n",
        "                        txt += pdf.pages[i].extract_text() or ''\n",
        "                    return txt\n",
        "                with ThreadPoolExecutor(max_workers=workers.value) as exe:\n",
        "                    for chunk in tqdm(exe.map(extract_range, range(0, total, chunk_size.value)), total=(total//chunk_size.value)+1):\n",
        "                        text_chunks.append(chunk)\n",
        "            print(f\"PDF dividido em {len(text_chunks)} chunks de até {chunk_size.value} páginas.\")\n",
        "        # FHIR JSONs\n",
        "        if fhir_upload.value:\n",
        "            print(\"\\n### FHIR Resources ###\")\n",
        "            for fname, info in fhir_upload.value.items():\n",
        "                try:\n",
        "                    data = json.loads(info['content'].decode('utf-8'))\n",
        "                    resource = construct_fhir_element(data.get('resourceType',''), data)\n",
        "                    print(f\"{fname}: Recurso {resource.resource_type}, versão {resource.meta.versionId if resource.meta else 'N/A'}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"{fname}: falha ao parsear FHIR - {e}\")\n",
        "        # Web scraping\n",
        "        urls = [u.strip() for u in url_list.value.splitlines() if u.strip()]\n",
        "        if urls:\n",
        "            print(\"\\n### Web Scraping ###\")\n",
        "            loop = asyncio.get_event_loop()\n",
        "            results = loop.run_until_complete(perform_scraping(urls))\n",
        "            for url, data in results.items():\n",
        "                print(f\"- {url}:\")\n",
        "                if 'error' in data:\n",
        "                    print(\"  Error:\", data['error'])\n",
        "                else:\n",
        "                    print(\"  Titles:\", data['titles'])\n",
        "                    print(\"  Paragraphs:\", data['paragraphs'])\n",
        "        print(\"\\n✅ Ingestão Avançada Concluída.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "391fc2a8",
      "metadata": {
        "id": "391fc2a8"
      },
      "source": [
        "Os dados ingeridos podem ser salvos para as próximas seções de pré-processamento, embeddings e além."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f36bcc17",
      "metadata": {
        "id": "f36bcc17"
      },
      "outputs": [],
      "source": [
        "# Instalação de dependências multimodais\n",
        "%pip install pydicom pillow torchaudio librosa openai-whisper ipywidgets --quiet\n",
        "\n",
        "import io\n",
        "import os\n",
        "import pydicom\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torchaudio\n",
        "import whisper\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "# Widgets\n",
        "img_upload = widgets.FileUpload(accept='.dcm,.png,.jpg,.jpeg', multiple=True, description='Upload Imagens')\n",
        "resize_slider = widgets.IntSlider(value=256, min=64, max=1024, step=64, description='Resize:')\n",
        "audio_upload = widgets.FileUpload(accept='.wav,.mp3,.flac', multiple=True, description='Upload Áudio')\n",
        "sample_rate = widgets.IntText(value=16000, description='Sample Rate:')\n",
        "transcribe_btn = widgets.Button(description='Processar Multimodal', button_style='success')\n",
        "out = widgets.Output()\n",
        "\n",
        "def process_multimodal(b):\n",
        "    with out:\n",
        "        clear_output()\n",
        "        # Process images\n",
        "        if img_upload.value:\n",
        "            print(\"### Imagens ###\")\n",
        "            for fname, info in img_upload.value.items():\n",
        "                content = info['content']\n",
        "                ext = fname.lower().split('.')[-1]\n",
        "                try:\n",
        "                    if ext == 'dcm':\n",
        "                        ds = pydicom.dcmread(io.BytesIO(content))\n",
        "                        img = ds.pixel_array\n",
        "                        img = Image.fromarray(img)\n",
        "                    else:\n",
        "                        img = Image.open(io.BytesIO(content))\n",
        "                    img = img.resize((resize_slider.value, resize_slider.value))\n",
        "                    arr = np.array(img)\n",
        "                    print(f\"{fname}: shape {arr.shape}, dtype {arr.dtype}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Erro ao processar imagem {fname}: {e}\")\n",
        "        # Process audio\n",
        "        if audio_upload.value:\n",
        "            print(\"\\n### Áudio ###\")\n",
        "            model = whisper.load_model('base')\n",
        "            for fname, info in audio_upload.value.items():\n",
        "                content = info['content']\n",
        "                try:\n",
        "                    # Save temp file\n",
        "                    tmp = f\"/tmp/{fname}\"\n",
        "                    with open(tmp, \"wb\") as f:\n",
        "                        f.write(content)\n",
        "                    waveform, sr = torchaudio.load(tmp)\n",
        "                    resampled = torchaudio.transforms.Resample(orig_freq=sr, new_freq=sample_rate.value)(waveform)\n",
        "                    print(f\"{fname}: original sr={sr}, new sr={sample_rate.value}, shape={resampled.shape}\")\n",
        "                    # Transcribe\n",
        "                    result = model.transcribe(tmp, fp16=False, language='pt')\n",
        "                    print(\"Transcrição:\", result['text'])\n",
        "                except Exception as e:\n",
        "                    print(f\"Erro ao processar áudio {fname}: {e}\")\n",
        "\n",
        "transcribe_btn.on_click(process_multimodal)\n",
        "display(widgets.VBox([\n",
        "    img_upload, resize_slider,\n",
        "    audio_upload, sample_rate,\n",
        "    transcribe_btn, out\n",
        "]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c29924a2",
      "metadata": {
        "id": "c29924a2"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import subprocess\n",
        "import torch\n",
        "import transformers\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "# Widgets\n",
        "prov = widgets.Dropdown(options=['OpenAI', 'HuggingFace'], description='Provedor:')\n",
        "run_btn = widgets.Button(description='▶️ Run Quickstart Test', button_style='success')\n",
        "out = widgets.Output()\n",
        "\n",
        "def quickstart_test(b):\n",
        "    with out:\n",
        "        clear_output()\n",
        "        print(\"### Environment Versions ###\")\n",
        "        print(f\"Python: {sys.version.split()[0]}\")\n",
        "        print(f\"PyTorch: {torch.__version__}\")\n",
        "        print(f\"Transformers: {transformers.__version__}\")\n",
        "        print(f\"CUDA Available: {torch.cuda.is_available()} ({torch.cuda.device_count()} devices)\")\n",
        "        print(\"\\n### Hello World Test ###\")\n",
        "        provider = prov.value\n",
        "        if provider == 'OpenAI':\n",
        "            import os, openai\n",
        "            key = os.getenv('OPENAI_API_KEY', '')\n",
        "            if not key:\n",
        "                print(\"⚠️ OPENAI_API_KEY não definido.\")\n",
        "            else:\n",
        "                try:\n",
        "                    resp = openai.ChatCompletion.create(\n",
        "                        model='gpt-3.5-turbo',\n",
        "                        messages=[{'role':'user','content':'Olá, mundo!'}]\n",
        "                    )\n",
        "                    print(\"Resposta OpenAI:\", resp.choices[0].message.content)\n",
        "                except Exception as e:\n",
        "                    print(\"Erro OpenAI:\", e)\n",
        "        else:\n",
        "            from transformers import pipeline\n",
        "            try:\n",
        "                pipe = pipeline('text-generation', model='gpt2')\n",
        "                outp = pipe('Hello, world!', max_length=20)\n",
        "                print(\"Resposta HF:\", outp[0]['generated_text'])\n",
        "            except Exception as e:\n",
        "                print(\"Erro HuggingFace:\", e)\n",
        "\n",
        "run_btn.on_click(quickstart_test)\n",
        "display(widgets.VBox([prov, run_btn, out]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47ebac96",
      "metadata": {
        "id": "47ebac96"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "# Credential widgets\n",
        "openai_key = widgets.Password(description='OpenAI Key:')\n",
        "hf_token   = widgets.Password(description='HF Token:')\n",
        "gemini_key = widgets.Password(description='Gemini Key:')\n",
        "vertex_key = widgets.Password(description='Vertex Key:')\n",
        "gcp_json   = widgets.FileUpload(accept='.json', multiple=False, description='GCP JSON')\n",
        "slack_url  = widgets.Text(description='Slack Webhook:')\n",
        "save_btn   = widgets.Button(description='Salvar Credenciais', button_style='success')\n",
        "out        = widgets.Output()\n",
        "\n",
        "def save_creds(b):\n",
        "    with out:\n",
        "        clear_output()\n",
        "        config = {\n",
        "            'OPENAI_API_KEY': openai_key.value,\n",
        "            'HF_TOKEN': hf_token.value,\n",
        "            'GEMINI_API_KEY': gemini_key.value,\n",
        "            'VERTEX_API_KEY': vertex_key.value,\n",
        "            'SLACK_WEBHOOK': slack_url.value\n",
        "        }\n",
        "        if gcp_json.value:\n",
        "            fname, info = next(iter(gcp_json.value.items()))\n",
        "            content = info['content'].decode('utf-8')\n",
        "            config['GCP_SA_JSON'] = json.loads(content)\n",
        "        # Save to config.json\n",
        "        with open('config.json','w',encoding='utf-8') as f:\n",
        "            json.dump(config, f, ensure_ascii=False, indent=2)\n",
        "        # Also set env vars in this session\n",
        "        for k, v in config.items():\n",
        "            os.environ[k] = v if isinstance(v, str) else json.dumps(v)\n",
        "        print(\"✅ Credenciais salvas em config.json e variáveis de ambiente definidas.\")\n",
        "\n",
        "save_btn.on_click(save_creds)\n",
        "display(openai_key, hf_token, gemini_key, vertex_key, gcp_json, slack_url, save_btn, out)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}